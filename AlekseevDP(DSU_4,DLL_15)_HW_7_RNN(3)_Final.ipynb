{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## AlekseevDP(DSU-4,DLL-15)_HW#7_Рекуррентные НС (Часть 3)\n",
        "\n",
        "Задание:\n",
        "- Возьмите англо-русскую пару фраз (https://www.manythings.org/anki/)\n",
        "- Обучите на них seq2seq по аналогии с занятием. Оцените полученное качество\n",
        "- Попробуйте добавить +1 рекуррентный слой в encoder и decoder\n",
        "- Попробуйте заменить GRU-ячейки на LSTM-ячейки\n",
        "\n",
        "Оцените качество во всех случаях\n",
        "\n"
      ],
      "metadata": {
        "id": "CtBL4V8vfsWC"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-YlRH3mQM9tf"
      },
      "source": [
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MIEGXF8oM9tt"
      },
      "source": [
        "from io import open\n",
        "import unicodedata\n",
        "import string\n",
        "import re\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "CUDA_LAUNCH_BLOCKING = \"1\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !wget https://www.manythings.org/anki/rus-eng.zip\n",
        "# !unzip rus-eng.zip"
      ],
      "metadata": {
        "id": "6jChhs7oEzGg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Файл https://www.manythings.org/anki/rus-eng.zip потребовал предобработки в программе Anki, а именно - удаления колонки со служебной информацией (номер  карточки перевода, автор и т.п.), иначе при загрузке корпуса возникала ошибка.Обработанный файл: eng-rus_prepared_full.txt"
      ],
      "metadata": {
        "id": "KXhRJh3Ohr8q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# переместим eng-rus_prepared_full.txt в каталог /data и переименуем в eng-rus.txt\n",
        "!mkdir '/content/data'\n",
        "!mv eng-rus_prepared_full.txt '/content/data/eng-rus.txt'"
      ],
      "metadata": {
        "id": "Ty4GvpoccaLh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "twIcAJnyRkW-",
        "outputId": "c1cb0b0c-5be4-4c01-90de-9504a62f5af2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!head -50 data/eng-rus.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Go.\tМарш!\n",
            "Hi.\tЗдравствуйте.\n",
            "Run!\tБеги!\n",
            "Run.\tБеги!\n",
            "Who?\tКто?\n",
            "Wow!\tВот это да!\n",
            "Duck!\tПригнись!\n",
            "Fire!\tОгонь!\n",
            "Help!\tПомогите!\n",
            "Hide.\tПрячься.\n",
            "Jump!\tПрыгай!\n",
            "Jump.\tПрыгай!\n",
            "Stay.\tОставайся.\n",
            "Stop!\tСтой!\n",
            "Wait!\tПодожди!\n",
            "Wait.\tЖдите.\n",
            "Do it.\tСделай это.\n",
            "Go on.\tПродолжай.\n",
            "Hello!\tЗдравствуйте.\n",
            "Hurry!\tПоспешите.\n",
            "I ran.\tЯ бежал.\n",
            "I see.\tПонимаю.\n",
            "I try.\tЯ пытаюсь.\n",
            "I won!\tЯ победил!\n",
            "Oh no!\tО нет!\n",
            "Relax.\tРасслабьтесь.\n",
            "Shoot!\tСтреляй!\n",
            "Smile.\tУлыбочка.\n",
            "Sorry?\tИзвините?\n",
            "Attack!\tВ атаку!\n",
            "Buy it.\tКупите её.\n",
            "Cheers!\tЗа ваше здоровье!\n",
            "Eat it.\tСъешь это.\n",
            "Eat up.\tДоедай.\n",
            "Freeze!\tНи с места!\n",
            "Get up.\tВставай.\n",
            "Go now.\tА теперь уходи.\n",
            "Got it!\tПонял!\n",
            "Got it?\tПонял?\n",
            "He ran.\tОн бежал.\n",
            "Hop in.\tЗалезай.\n",
            "Hug me.\tОбними меня.\n",
            "I fell.\tЯ упал.\n",
            "I knit.\tЯ вяжу.\n",
            "I know.\tЯ знаю.\n",
            "I left.\tЯ ушёл.\n",
            "I lied.\tЯ солгал.\n",
            "I lost.\tЯ проиграл.\n",
            "I paid.\tЯ заплатил.\n",
            "I pass.\tЯ пас.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kyNnJyruM9t1"
      },
      "source": [
        "SOS_token = 0\n",
        "EOS_token = 1\n",
        "\n",
        "\n",
        "class Lang:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.word2index = {}\n",
        "        self.word2count = {}\n",
        "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
        "        self.n_words = 2  # Count SOS and EOS\n",
        "\n",
        "    def addSentence(self, sentence):\n",
        "        for word in sentence.split(' '):\n",
        "            self.addWord(word)\n",
        "\n",
        "    def addWord(self, word):\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = self.n_words\n",
        "            self.word2count[word] = 1\n",
        "            self.index2word[self.n_words] = word\n",
        "            self.n_words += 1\n",
        "        else:\n",
        "            self.word2count[word] += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Через библиотеку Unicodedata не удалось \"забороть\" кодировку предложений на русском языке, хотя кодировка UTF-8 включает в себя в т.ч. русские буквы (ошибка не возникала, но русскоязычная часть корпуса получалась всегда пустой - 0 фраз). Поэтому нашел альтернативный вариант - русские фразы переводить в транслит с использованием другой библиотеки (Unidecode):"
      ],
      "metadata": {
        "id": "-Zw-0lHtV7ga"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install Unidecode"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y89hifz8H1sH",
        "outputId": "f1ba871c-4663-4ea3-f7ae-ff099d1e0b16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting Unidecode\n",
            "  Downloading Unidecode-1.3.6-py3-none-any.whl (235 kB)\n",
            "\u001b[K     |████████████████████████████████| 235 kB 15.3 MB/s \n",
            "\u001b[?25hInstalling collected packages: Unidecode\n",
            "Successfully installed Unidecode-1.3.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from unidecode import unidecode"
      ],
      "metadata": {
        "id": "6Z3NHaB8HsFs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FXKs8j4bM9t6"
      },
      "source": [
        "def unicodeToAscii(s):\n",
        "    return ''.join(\n",
        "        # c for c in unicodedata.normalize('NFD', s)  # Turn a Unicode string to plain ASCII, \n",
        "        # if unicodedata.category(c) != 'Mn'          # thanks to http://stackoverflow.com/a/518232/2809427\n",
        "        c for c in unidecode(s)\n",
        ")\n",
        "\n",
        "# Lowercase, trim, and remove non-letter characters\n",
        "def normalizeString(s):\n",
        "    s = unicodeToAscii(s.lower().strip())\n",
        "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
        "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
        "    return s"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D8T4VxZeM9t-"
      },
      "source": [
        "def readLangs(lang1, lang2, reverse=False):\n",
        "    print(\"Reading lines...\")\n",
        "\n",
        "    # Read the file and split into lines\n",
        "    lines = open('data/%s-%s.txt' % (lang1, lang2), encoding='utf-8').\\\n",
        "        read().strip().split('\\n')\n",
        "\n",
        "    # Split every line into pairs and normalize\n",
        "    pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
        "\n",
        "    # Reverse pairs, make Lang instances\n",
        "    if reverse:\n",
        "        pairs = [list(reversed(p)) for p in pairs]\n",
        "        input_lang = Lang(lang2)\n",
        "        output_lang = Lang(lang1)\n",
        "    else:\n",
        "        input_lang = Lang(lang1)\n",
        "        output_lang = Lang(lang2)\n",
        "\n",
        "    return input_lang, output_lang, pairs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## На \"типовых\" префиксах начала предложений (из туториала/лекции) модель Seq2seq на 1-слойных GRU-энкодере/декодере показывала хорошую точность предсказаний (лосс ниже 1.0), т.к. корпус был небольшой и предложения короткие. Поэтому с целью эксперимента задача была усложнена: увеличена длина предложения до 100 символов, а также добавлены новые префиксы начала предложений. Итоговый корпус для обучения модели составил 29743 фразы (см.ниже)."
      ],
      "metadata": {
        "id": "y5XjTYr3jI2i"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eBOwgEBdM9uB"
      },
      "source": [
        "MAX_LENGTH = 100\n",
        "\n",
        "eng_prefixes = (\n",
        "    \"i am\", \"i m\",\n",
        "    \"he is\", \"he s\",\n",
        "    \"she is\", \"she s\",\n",
        "    \"you are\", \"you re\",\n",
        "    \"we are\", \"we re\",\n",
        "    \"they are\", \"they re\",\n",
        "    \"i can\", \"i can t\",\n",
        "    \"that is\", \"that s\",\n",
        "    \"we will\", \"we ll\",\n",
        "    \"i was\", \"he was\", \"she was\",\n",
        "    \"do you\", \"are you\",\n",
        "    \"i have\", \"i ve\", \"i had\", \"i d\",\n",
        "    \"what is\", \"what s\", \"what was\",\n",
        "    \"what do\", \"what did\",\n",
        "    \"we have\", \"we ve\",\n",
        "    \"i do\", \"i don t\",\n",
        "    \"did\", \"i ll\",\n",
        "    \"there is\", \"there s\",\n",
        "    \"how\", \"when\"\n",
        ")\n",
        "\n",
        "def filterPair(p):\n",
        "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
        "        len(p[1].split(' ')) < MAX_LENGTH and \\\n",
        "        p[1].startswith(eng_prefixes)\n",
        "\n",
        "def filterPairs(pairs):\n",
        "    return [pair for pair in pairs if filterPair(pair)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6dZOGjd5M9uE",
        "outputId": "8d447b8c-c382-4fc6-92ea-644e67b5392f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def prepareData(lang1, lang2, reverse=False):\n",
        "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n",
        "    print(\"Read %s sentence pairs\" % len(pairs))\n",
        "    pairs = filterPairs(pairs)\n",
        "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
        "    print(\"Counting words...\")\n",
        "    for pair in pairs:\n",
        "        input_lang.addSentence(pair[0])\n",
        "        output_lang.addSentence(pair[1])\n",
        "    print(\"Counted words:\")\n",
        "    print(input_lang.name, input_lang.n_words)\n",
        "    print(output_lang.name, output_lang.n_words)\n",
        "    return input_lang, output_lang, pairs\n",
        "\n",
        "input_lang, output_lang, pairs = prepareData('eng', 'rus', True)\n",
        "print(random.choice(pairs))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading lines...\n",
            "Read 746410 sentence pairs\n",
            "Trimmed to 29743 sentence pairs\n",
            "Counting words...\n",
            "Counted words:\n",
            "rus 13446\n",
            "eng 6084\n",
            "['kogda budesh znat soobshchi mne .', 'when you know let me know .']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(random.choice(pairs))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P9jQLXMv4VSY",
        "outputId": "965e5b05-c082-45e6-ca26-fc37da8cacba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['ty chto sobiraesh sia sidet zdes nichego ne delaia ?', 'are you just going to sit here and not do anything ?']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vgtWqznCM9uH"
      },
      "source": [
        "The Encoder (1-layer GRU)\n",
        "-----------\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m9vm9QBWM9uI"
      },
      "source": [
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        output = embedded\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FwLTlgSyM9uK"
      },
      "source": [
        "The Decoder (1-layer GRU)\n",
        "-----------\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PFbuUL1LM9uL"
      },
      "source": [
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size):\n",
        "        super(DecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        output = self.embedding(input).view(1, 1, -1)\n",
        "        output = F.relu(output)\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        output = self.softmax(self.out(output[0]))\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z6gGPtXFM9uQ"
      },
      "source": [
        "def indexesFromSentence(lang, sentence):\n",
        "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
        "\n",
        "\n",
        "def tensorFromSentence(lang, sentence):\n",
        "    indexes = indexesFromSentence(lang, sentence)\n",
        "    indexes.append(EOS_token)\n",
        "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
        "\n",
        "\n",
        "def tensorsFromPair(pair):\n",
        "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
        "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
        "    return (input_tensor, target_tensor)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Fn8VDv8M9uS"
      },
      "source": [
        "teacher_forcing_ratio = 0.5\n",
        "\n",
        "\n",
        "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
        "    encoder_hidden = encoder.initHidden()\n",
        "\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "\n",
        "    input_length = input_tensor.size(0)\n",
        "    target_length = target_tensor.size(0)\n",
        "\n",
        "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "    loss = 0\n",
        "\n",
        "    for ei in range(input_length):\n",
        "        encoder_output, encoder_hidden = encoder(\n",
        "            input_tensor[ei], encoder_hidden)\n",
        "        encoder_outputs[ei] = encoder_output[0, 0]\n",
        "\n",
        "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
        "\n",
        "    decoder_hidden = encoder_hidden\n",
        "\n",
        "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
        "\n",
        "    if use_teacher_forcing:\n",
        "        # Teacher forcing: Feed the target as the next input\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden = decoder(\n",
        "                decoder_input, decoder_hidden)\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            decoder_input = target_tensor[di]  # Teacher forcing\n",
        "\n",
        "    else:\n",
        "        # Without teacher forcing: use its own predictions as the next input\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden = decoder(\n",
        "                decoder_input, decoder_hidden)\n",
        "            topv, topi = decoder_output.topk(1)\n",
        "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
        "\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            if decoder_input.item() == EOS_token:\n",
        "                break\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    return loss.item() / target_length"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EKsdwPmSM9uU"
      },
      "source": [
        "import time\n",
        "import math\n",
        "\n",
        "def asMinutes(s):\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)\n",
        "\n",
        "def timeSince(since, percent):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    es = s / (percent)\n",
        "    rs = es - s\n",
        "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C_z_k5IiM9uX"
      },
      "source": [
        "def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):\n",
        "    start = time.time()\n",
        "    plot_losses = []\n",
        "    print_loss_total = 0  # Reset every print_every\n",
        "    plot_loss_total = 0  # Reset every plot_every\n",
        "\n",
        "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
        "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
        "    training_pairs = [tensorsFromPair(random.choice(pairs))\n",
        "                      for i in range(n_iters)]\n",
        "    criterion = nn.NLLLoss()\n",
        "\n",
        "    for iter in range(1, n_iters + 1):\n",
        "        training_pair = training_pairs[iter - 1]\n",
        "        input_tensor = training_pair[0]\n",
        "        target_tensor = training_pair[1]\n",
        "\n",
        "        loss = train(input_tensor, target_tensor, encoder,\n",
        "                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
        "        print_loss_total += loss\n",
        "        plot_loss_total += loss\n",
        "\n",
        "        if iter % print_every == 0:\n",
        "            print_loss_avg = print_loss_total / print_every\n",
        "            print_loss_total = 0\n",
        "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
        "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
        "\n",
        "        if iter % plot_every == 0:\n",
        "            plot_loss_avg = plot_loss_total / plot_every\n",
        "            plot_losses.append(plot_loss_avg)\n",
        "            plot_loss_total = 0\n",
        "\n",
        "    showPlot(plot_losses)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0JXG-RzCM9uZ"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.switch_backend('agg')\n",
        "import matplotlib.ticker as ticker\n",
        "import numpy as np\n",
        "\n",
        "def showPlot(points):\n",
        "    plt.figure()\n",
        "    fig, ax = plt.subplots()\n",
        "    # this locator puts ticks at regular intervals\n",
        "    loc = ticker.MultipleLocator(base=0.2)\n",
        "    ax.yaxis.set_major_locator(loc)\n",
        "    plt.plot(points)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Bxf45h6M9ud"
      },
      "source": [
        "def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
        "    with torch.no_grad():\n",
        "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
        "        input_length = input_tensor.size()[0]\n",
        "        encoder_hidden = encoder.initHidden()\n",
        "\n",
        "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "        for ei in range(input_length):\n",
        "            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n",
        "                                                     encoder_hidden)\n",
        "            encoder_outputs[ei] += encoder_output[0, 0]\n",
        "\n",
        "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
        "\n",
        "        decoder_hidden = encoder_hidden\n",
        "\n",
        "        decoded_words = []\n",
        "\n",
        "        for di in range(max_length):\n",
        "            decoder_output, decoder_hidden = decoder(\n",
        "                decoder_input, decoder_hidden)\n",
        "            topv, topi = decoder_output.data.topk(1)\n",
        "            if topi.item() == EOS_token:\n",
        "                decoded_words.append('<EOS>')\n",
        "                break\n",
        "            else:\n",
        "                decoded_words.append(output_lang.index2word[topi.item()])\n",
        "\n",
        "            decoder_input = topi.squeeze().detach()\n",
        "\n",
        "        return decoded_words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1qUmQIGwM9uf"
      },
      "source": [
        "def evaluateRandomly(encoder, decoder, n=10):\n",
        "    for i in range(n):\n",
        "        pair = random.choice(pairs)\n",
        "        print('>', pair[0])\n",
        "        print('=', pair[1])\n",
        "        output_words = evaluate(encoder, decoder, pair[0])\n",
        "        output_sentence = ' '.join(output_words)\n",
        "        print('<', output_sentence)\n",
        "        print('')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Обучим  модель Seq2seq с использованием 1-слойных GRU для энкодера&декодера.С целью повышения качества предсказаний количество эпох увеличим вдвое (с 75000 до 150000)"
      ],
      "metadata": {
        "id": "GXRgoO7Nitdl"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s_56t10oM9uh",
        "outputId": "fc0ede62-3df5-4a0a-e60e-4331536c421c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "hidden_size = 256\n",
        "encoder1 = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
        "decoder1 = DecoderRNN(hidden_size, output_lang.n_words).to(device)\n",
        "\n",
        "trainIters(encoder1, decoder1, 150000, print_every=5000) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1m 0s (- 29m 0s) (5000 3%) 3.7474\n",
            "1m 51s (- 26m 4s) (10000 6%) 3.2412\n",
            "2m 44s (- 24m 43s) (15000 10%) 2.9723\n",
            "3m 37s (- 23m 33s) (20000 13%) 2.7699\n",
            "4m 30s (- 22m 31s) (25000 16%) 2.6018\n",
            "5m 22s (- 21m 31s) (30000 20%) 2.4844\n",
            "6m 15s (- 20m 33s) (35000 23%) 2.3738\n",
            "7m 8s (- 19m 37s) (40000 26%) 2.2868\n",
            "8m 1s (- 18m 42s) (45000 30%) 2.2007\n",
            "8m 54s (- 17m 48s) (50000 33%) 2.1600\n",
            "9m 46s (- 16m 53s) (55000 36%) 2.0674\n",
            "10m 39s (- 15m 59s) (60000 40%) 1.9800\n",
            "11m 33s (- 15m 6s) (65000 43%) 1.9497\n",
            "12m 26s (- 14m 12s) (70000 46%) 1.9243\n",
            "13m 19s (- 13m 19s) (75000 50%) 1.8637\n",
            "14m 13s (- 12m 26s) (80000 53%) 1.8398\n",
            "15m 6s (- 11m 32s) (85000 56%) 1.7801\n",
            "15m 58s (- 10m 39s) (90000 60%) 1.7514\n",
            "16m 51s (- 9m 45s) (95000 63%) 1.7061\n",
            "17m 44s (- 8m 52s) (100000 66%) 1.7122\n",
            "18m 37s (- 7m 58s) (105000 70%) 1.6542\n",
            "19m 30s (- 7m 5s) (110000 73%) 1.6396\n",
            "20m 23s (- 6m 12s) (115000 76%) 1.5780\n",
            "21m 17s (- 5m 19s) (120000 80%) 1.5571\n",
            "22m 9s (- 4m 25s) (125000 83%) 1.5464\n",
            "23m 2s (- 3m 32s) (130000 86%) 1.4969\n",
            "23m 55s (- 2m 39s) (135000 90%) 1.4576\n",
            "24m 48s (- 1m 46s) (140000 93%) 1.4540\n",
            "25m 41s (- 0m 53s) (145000 96%) 1.4499\n",
            "26m 34s (- 0m 0s) (150000 100%) 1.4227\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xEoEylSyM9uj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a92f4313-f663-4d2e-f9fd-a0b78ee3c9c8"
      },
      "source": [
        "evaluateRandomly(encoder1, decoder1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> ona vsio eshchio ego liubit .\n",
            "= she still loves him .\n",
            "< she still loves him . <EOS>\n",
            "\n",
            "> ona s nim surova .\n",
            "= she is hard on him .\n",
            "< she is hard on him . <EOS>\n",
            "\n",
            "> ty kogo nibud tam videl ?\n",
            "= did you see anybody there ?\n",
            "< did you see anyone there ? <EOS>\n",
            "\n",
            "> kak liudi tochili karandashi do izobreteniia tochilki ?\n",
            "= how did people sharpen their pencils before the invention of the pencil sharpener ?\n",
            "< how did the get get the of get ? <EOS>\n",
            "\n",
            "> vy izuchaete frantsuzskii ?\n",
            "= do you study french ?\n",
            "< do you study french ? <EOS>\n",
            "\n",
            "> mne nado znat seichas .\n",
            "= i have to know now .\n",
            "< i have to know now . <EOS>\n",
            "\n",
            "> ia sdelal eto dlia nas .\n",
            "= i did that for us .\n",
            "< i did it for for . <EOS>\n",
            "\n",
            "> on istekal krov iu ot ran .\n",
            "= he was bleeding from his wounds .\n",
            "< he is from from from . . <EOS>\n",
            "\n",
            "> kogda ty tuda doberesh sia ?\n",
            "= when are you going to get there ?\n",
            "< when do you get there ? <EOS>\n",
            "\n",
            "> ia sobiralsia zadat tebe tot zhe vopros .\n",
            "= i was about to ask you the same question .\n",
            "< i was the to ask you to ask . <EOS>\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Мы видим, что визуально качество перевода не очень хорошее и лосс гораздо больше 1.0. Возможно, при увеличении количества эпох ещё в несколько раз качество перевода удастся существенно улучшить, но эта операция очень ресурсоемкая: на GPU Google Colab (был выделен Tesla T4) обучение модели заняло ок.30 минут. Попробуем добавить по одному рекуррентному слою GRU в Encoder и Decoder (просто переопределим классы)."
      ],
      "metadata": {
        "id": "LPOrjm8a2jEu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## 2-layers GRU @Encoder\n",
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, n_layers):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        output = embedded\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        # return torch.zeros(1, 1, self.hidden_size, device=device)\n",
        "        return torch.zeros(2, 1, self.hidden_size, device=device)"
      ],
      "metadata": {
        "id": "N4S01SsB2VGp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## 2-layers GRU @Decoder\n",
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size, n_layers):\n",
        "        super(DecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        output = self.embedding(input).view(1, 1, -1)\n",
        "        output = F.relu(output)\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        output = self.softmax(self.out(output[0]))\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        # return torch.zeros(1, 1, self.hidden_size, device=device)\n",
        "        return torch.zeros(2, 1, self.hidden_size, device=device)"
      ],
      "metadata": {
        "id": "-j0emwd94BY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hidden_size = 256\n",
        "num_layers = 2\n",
        "encoder1 = EncoderRNN(input_lang.n_words, hidden_size, num_layers).to(device)\n",
        "decoder1 = DecoderRNN(hidden_size, output_lang.n_words, num_layers).to(device)\n",
        "\n",
        "trainIters(encoder1, decoder1, 150000, print_every=5000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HRLSiExI4n_g",
        "outputId": "5bad93a9-507c-4bb9-bd69-087d84571ba7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1m 10s (- 34m 18s) (5000 3%) 3.7949\n",
            "2m 12s (- 31m 1s) (10000 6%) 3.2709\n",
            "3m 14s (- 29m 12s) (15000 10%) 3.0179\n",
            "4m 17s (- 27m 51s) (20000 13%) 2.8252\n",
            "5m 20s (- 26m 41s) (25000 16%) 2.6823\n",
            "6m 23s (- 25m 32s) (30000 20%) 2.5338\n",
            "7m 25s (- 24m 25s) (35000 23%) 2.4270\n",
            "8m 29s (- 23m 19s) (40000 26%) 2.3474\n",
            "9m 31s (- 22m 13s) (45000 30%) 2.2012\n",
            "10m 34s (- 21m 8s) (50000 33%) 2.1370\n",
            "11m 37s (- 20m 4s) (55000 36%) 2.1030\n",
            "12m 39s (- 18m 59s) (60000 40%) 2.0296\n",
            "13m 42s (- 17m 55s) (65000 43%) 1.9408\n",
            "14m 45s (- 16m 52s) (70000 46%) 1.9263\n",
            "15m 48s (- 15m 48s) (75000 50%) 1.8574\n",
            "16m 51s (- 14m 45s) (80000 53%) 1.7943\n",
            "17m 54s (- 13m 41s) (85000 56%) 1.7575\n",
            "18m 57s (- 12m 38s) (90000 60%) 1.7528\n",
            "20m 0s (- 11m 35s) (95000 63%) 1.6581\n",
            "21m 3s (- 10m 31s) (100000 66%) 1.6490\n",
            "22m 6s (- 9m 28s) (105000 70%) 1.6144\n",
            "23m 10s (- 8m 25s) (110000 73%) 1.6010\n",
            "24m 13s (- 7m 22s) (115000 76%) 1.5472\n",
            "25m 16s (- 6m 19s) (120000 80%) 1.5584\n",
            "26m 19s (- 5m 15s) (125000 83%) 1.5142\n",
            "27m 22s (- 4m 12s) (130000 86%) 1.5065\n",
            "28m 26s (- 3m 9s) (135000 90%) 1.4539\n",
            "29m 29s (- 2m 6s) (140000 93%) 1.4945\n",
            "30m 32s (- 1m 3s) (145000 96%) 1.4015\n",
            "31m 35s (- 0m 0s) (150000 100%) 1.3766\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evaluateRandomly(encoder1, decoder1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g7pj6e2UKU0E",
        "outputId": "355ddc9b-5067-47e2-c7f0-2ab999d5ccff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> ty deistvitel no zdorovo vygliadish .\n",
            "= you really do look great .\n",
            "< you really are look beautiful . <EOS>\n",
            "\n",
            "> mne skazali chto tom uekhal za granitsu .\n",
            "= i was told that tom went abroad .\n",
            "< i was told tom was interested in . <EOS>\n",
            "\n",
            "> kak skoro ty mozhesh zakonchit eti risunki ?\n",
            "= how quickly can you finish these pictures ?\n",
            "< how soon can you be such my hands ? <EOS>\n",
            "\n",
            "> u tebia eshchio est voprosy ?\n",
            "= do you still have questions ?\n",
            "< do you have any more ? <EOS>\n",
            "\n",
            "> khotite chtoby ia ostalsia zdes ?\n",
            "= do you want me to stay here ?\n",
            "< do you want me to stay here ? <EOS>\n",
            "\n",
            "> tebe ponravilos ili net ?\n",
            "= did you like that or not ?\n",
            "< did you like that or not ? <EOS>\n",
            "\n",
            "> kak chasto ty pol zuesh sia kameroi na svoiom smartfone ?\n",
            "= how often do you use the camera on your smartphone ?\n",
            "< how often do you use your your ? <EOS>\n",
            "\n",
            "> ty znala chto u tebia krasivye glaza ?\n",
            "= did you know you have pretty eyes ?\n",
            "< did you know that you you you ? ? ? <EOS>\n",
            "\n",
            "> ia gotov sdelat chto ugodno chtoby pomoch tebe .\n",
            "= i am ready to do anything to help you .\n",
            "< i am ready to do anything to help help . <EOS>\n",
            "\n",
            "> on poslednii chelovek s kem ia khochu razgovarivat .\n",
            "= he is the last man that i want to talk with .\n",
            "< he is the last person i want to go to see . <EOS>\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Мы видим, что качество переведо с использованием 2х-слойных GRU для энкодера/декодера несущественно улучшилось (лосс=1.3766 на 2х-слойных против лосса=1.4227 на 1-слойных). Время обучения модели при этом практически не увеличилось.\n",
        "\n",
        "## Попробуем теперь заменить слои GRU на LSTM.\n",
        "[На материалах лекции замену на LSTM сделать не удалось. Поэтому в качестве основы были взяты материалы с https://github.com/bentrevett/pytorch-seq2seq , 1 - Sequence to Sequence Learning with Neural Networks.ipynb  \n",
        "Тем не менее, возникли ошибки, попытки исправления которых пока не дали результата...]"
      ],
      "metadata": {
        "id": "jQr9JJYeRsyF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderLSTM(nn.Module):\n",
        "    def __init__(self, input_dim, emb_dim, hidden_size, n_layers, dropout):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.emb_dim = emb_dim\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
        "        self.rnn = nn.LSTM(emb_dim, hidden_size, n_layers, dropout = dropout)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, input):\n",
        "        embedded = self.dropout(self.embedding(input).view(1, 1, -1))\n",
        "        output, (hidden, cell) = self.rnn(embedded)\n",
        "        return hidden, cell\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)"
      ],
      "metadata": {
        "id": "Jssuo0LPSmyw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderLSTM(nn.Module):\n",
        "    def __init__(self, output_dim, emb_dim, hidden_size, n_layers, dropout):\n",
        "        super().__init__()\n",
        "        self.output_dim = output_dim\n",
        "        self.hidden_size = hidden_size\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
        "        self.rnn = nn.LSTM(emb_dim, hidden_size, n_layers, dropout = dropout)\n",
        "        self.fc_out = nn.Linear(hidden_size, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        " \n",
        "    def forward(self, input, hidden, cell):\n",
        "        input = input.unsqueeze(0)\n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n",
        "        prediction = self.fc_out(output.squeeze(0))\n",
        "        return prediction, hidden, cell\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)"
      ],
      "metadata": {
        "id": "KQGLSgqLS4VE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "INPUT_DIM = input_lang.n_words\n",
        "OUTPUT_DIM = output_lang.n_words\n",
        "ENC_EMB_DIM = 256\n",
        "DEC_EMB_DIM = 256\n",
        "HID_DIM = 512\n",
        "N_LAYERS = 2\n",
        "ENC_DROPOUT = 0.5\n",
        "DEC_DROPOUT = 0.5\n",
        "\n",
        "encoder1 = EncoderLSTM(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT)\n",
        "decoder1 = DecoderLSTM(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT)\n",
        "\n",
        "trainIters(encoder1, decoder1, 75000, print_every=5000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        },
        "id": "PK5J38P3XE_m",
        "outputId": "569c31e2-6058-48ba-bd22-bc3cd1d963d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-80-8d09f4821a4f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mdecoder1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDecoderLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mOUTPUT_DIM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDEC_EMB_DIM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHID_DIM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN_LAYERS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDEC_DROPOUT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mtrainIters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m75000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-54-29eaa6fdc28a>\u001b[0m in \u001b[0;36mtrainIters\u001b[0;34m(encoder, decoder, n_iters, print_every, plot_every, learning_rate)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         loss = train(input_tensor, target_tensor, encoder,\n\u001b[0;32m---> 19\u001b[0;31m                      decoder, encoder_optimizer, decoder_optimizer, criterion)\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mprint_loss_total\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mplot_loss_total\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-52-c2cfa65ad37c>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mei\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         encoder_output, encoder_hidden = encoder(\n\u001b[0;32m---> 19\u001b[0;31m             input_tensor[ei], encoder_hidden)\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mei\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: forward() takes 2 positional arguments but 3 were given"
          ]
        }
      ]
    }
  ]
}