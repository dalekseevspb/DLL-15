{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## AlekseevDP(DSU-4, DLL-15)_HW#5_Рекуррентные НС(Часть 1)_RNN1\n",
        "\n",
        "Задание 1. Обучите нейронную сеть решать шифр Цезаря:\n",
        "1. Напишите алгоритм шифра Цезаря для генерации выборки (сдвиг на К каждой\n",
        "буквы. Например, при сдвиге на 2 буква “А” переходит в букву “В” и тп)\n",
        "2. Сделайте нейронную сеть\n",
        "3. Обучите её (вход - зашифрованная фраза, выход - дешифрованная фраза)\n",
        "4. Проверьте качество\n",
        "\n",
        "Задание 2.\n",
        "Выполните практическую работу из лекционного ноутбука:\n",
        "1. Постройте RNN-ячейку на основе полносвязных слоёв\n",
        "2. Примените построенную ячейку для генерации текста с выражениями героев\n",
        "сериала \"Симпсоны"
      ],
      "metadata": {
        "id": "p1AhlKqaJbBQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "eZtwFoKZJaHu"
      },
      "outputs": [],
      "source": [
        "# Задание 1. Обучите нейронную сеть решать шифр Цезаря.\n",
        "import pandas as pd\n",
        "import torch\n",
        "import time\n",
        "import math\n",
        "\n",
        "from random import shuffle"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Зададим некоторые параметры\n",
        "BATCH_SIZE = 10\n",
        "STRING_SIZE = 300\n",
        "NUM_EPOCHS = 300\n",
        "LEARNING_RATE = 0.05\n",
        "FILE_NAME = \"/content/onegin.txt\"\n",
        "CAESAR_OFFSET = 3 # смещение для шифра Цезаря"
      ],
      "metadata": {
        "id": "ELWVXJqu7elk"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## выведем активное устройство для обучения модели\n",
        "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print (DEVICE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y27uSrQEnqoa",
        "outputId": "d62a5e6d-675f-4335-cdde-64b64a95d978"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-03-18T11:25:36.001070Z",
          "iopub.status.busy": "2021-03-18T11:25:36.000289Z",
          "iopub.status.idle": "2021-03-18T11:25:36.200226Z",
          "shell.execute_reply": "2021-03-18T11:25:36.201255Z"
        },
        "papermill": {
          "duration": 0.223214,
          "end_time": "2021-03-18T11:25:36.201519",
          "exception": false,
          "start_time": "2021-03-18T11:25:35.978305",
          "status": "completed"
        },
        "tags": [],
        "id": "provincial-omega",
        "outputId": "9a802e6c-3a94-481e-aad5-e5b9b6be60cc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Алфавит, сформированный из корпуса:\n",
            " Е в г е н и й   О \n",
            " А л к с а д р С ч П у ш К я о ь Р м т х .   « » ( 1 8 2 3 – ) з ы п б 9 5 : Н В Г Б , э ц ж X I P t r i d e v a n l c o p u s ? ’ g q f m b - T [ щ ё ю И ф ] \t H Х З Д ; У М ! Т Л # _ M A Ч V y — ъ Ю Э Ф … Я Ш 4 h Ц 6 Ж B k C L 7 0 O N Y E / w W Q\n",
            " Итого 134 символа\n"
          ]
        }
      ],
      "source": [
        "# Зададим класс создания алфавита из исходного файла (корпуса)\n",
        "# (т.е. в алфавит будут входить ВСЕ возможные символы из корпуса, включая цифры, знаки препинания и спецсимволы)\n",
        "class Alphabet(object):\n",
        "\n",
        "    def __init__(self):\n",
        "        self.letters = \"\"\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.letters)\n",
        "\n",
        "    def __contains__(self, item):\n",
        "        return item in self.letters\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        if isinstance(item, int):\n",
        "            return self.letters[item % len(self.letters)]\n",
        "        elif isinstance(item, str):\n",
        "            return self.letters.find(item)\n",
        "\n",
        "    def __str__(self):\n",
        "        letters = \" \".join(self.letters)\n",
        "        return f\"Алфавит, сформированный из корпуса:\\n {letters}\\n Итого {len(self)} символа\"\n",
        "\n",
        "    def load_from_file(self, file_path):\n",
        "        with open(file_path) as file:\n",
        "            while True:\n",
        "                text = file.read(STRING_SIZE)\n",
        "                if not text:\n",
        "                    break\n",
        "                for ch in text:\n",
        "                    if ch not in self.letters:  # если прочитанный символ отсутствует в алфавите, то добавляем его в алфавите \n",
        "                        self.letters += ch\n",
        "        return self\n",
        "\n",
        "\n",
        "ALPHABET = Alphabet().load_from_file(FILE_NAME)\n",
        "print(ALPHABET)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# класс создания датасетов \n",
        "class SentenceDataset(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, raw_data, alphabet):\n",
        "        super().__init__()\n",
        "        self._len = len(raw_data)\n",
        "        self.y = torch.tensor(\n",
        "            [[alphabet[ch] for ch in line] for line in raw_data]\n",
        "        ).to(DEVICE)\n",
        "        self.x = torch.tensor(\n",
        "            [[i + CAESAR_OFFSET for i in line] for line in self.y]\n",
        "        ).to(DEVICE)\n",
        "    \n",
        "    def __len__(self):\n",
        "        return self._len\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.x[idx], self.y[idx]"
      ],
      "metadata": {
        "id": "j0upHxmyr6Me"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# функция нарезки массива исходного текста на куски размером STRING_SIZE\n",
        "def get_text_array(file_path, step):\n",
        "    text_array = []\n",
        "    with open(file_path) as file:\n",
        "        while True:\n",
        "            text = file.read(STRING_SIZE)\n",
        "            if not text:\n",
        "                break\n",
        "            text_array.append(text)\n",
        "    del text_array[-1]\n",
        "    return text_array"
      ],
      "metadata": {
        "id": "sDjKskMbsAOd"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# заполняем трейн- и тест-выборки\n",
        "raw_data = get_text_array(FILE_NAME, STRING_SIZE)\n",
        "shuffle(raw_data)\n",
        "\n",
        "_10_percent = math.ceil(len(raw_data) * 0.1)\n",
        "val_data = raw_data[:_10_percent]\n",
        "raw_data = raw_data[_10_percent:]\n",
        "\n",
        "_20_percent = math.ceil(len(raw_data) * 0.2)\n",
        "test_data = raw_data[:_20_percent]\n",
        "train_data = raw_data[_20_percent:]\n",
        "\n",
        "Y_val = torch.tensor([[ALPHABET[ch] for ch in line] for line in val_data])\n",
        "X_val = torch.tensor([[i + CAESAR_OFFSET for i in line] for line in Y_val])\n",
        "\n",
        "train_dl = torch.utils.data.DataLoader(\n",
        "    SentenceDataset(\n",
        "        train_data, ALPHABET\n",
        "    ),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    drop_last=True\n",
        ")\n",
        "test_dl = torch.utils.data.DataLoader(\n",
        "    SentenceDataset(\n",
        "        test_data, ALPHABET\n",
        "    ),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    drop_last=True\n",
        ")"
      ],
      "metadata": {
        "id": "l7Oc_qKRsBUH"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-03-18T11:25:51.794474Z",
          "iopub.status.busy": "2021-03-18T11:25:51.793530Z",
          "iopub.status.idle": "2021-03-18T11:25:51.796894Z",
          "shell.execute_reply": "2021-03-18T11:25:51.796274Z"
        },
        "papermill": {
          "duration": 0.023827,
          "end_time": "2021-03-18T11:25:51.797037",
          "exception": false,
          "start_time": "2021-03-18T11:25:51.773210",
          "status": "completed"
        },
        "tags": [],
        "id": "initial-ebony"
      },
      "outputs": [],
      "source": [
        "# класс инициализации рекуррентной НС.\n",
        "# Используем размерность алфавита, загруженного из исходного корпуса (134 символа), \n",
        "# а также кол-во символов смещения шифра (CAESAR_OFFSET)\n",
        "class RNNModel(torch.nn.Module):\n",
        "    \n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.embed = torch.nn.Embedding(len(ALPHABET) + CAESAR_OFFSET, 134)\n",
        "        self.rnn = torch.nn.RNN(134, 134, batch_first=True)\n",
        "        self.linear = torch.nn.Linear(134, len(ALPHABET) + CAESAR_OFFSET)\n",
        "\n",
        "    def forward(self, sentence, state=None):\n",
        "        embed = self.embed(sentence)\n",
        "        o, h = self.rnn(embed)\n",
        "        return self.linear(o)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# создаем модель, задаем функцию потерь и оптимизатор\n",
        "model = RNNModel().to(DEVICE)\n",
        "loss = torch.nn.CrossEntropyLoss().to(DEVICE)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE)"
      ],
      "metadata": {
        "id": "TYeknZ6qmZJ7"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# обучаем модель\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    train_loss, train_acc, iter_num = .0, .0, .0\n",
        "    start_epoch_time = time.time()\n",
        "    model.train()\n",
        "    for x_in, y_in in train_dl:\n",
        "        x_in = x_in\n",
        "        y_in = y_in.view(1, -1).squeeze()\n",
        "        optimizer.zero_grad()\n",
        "        out = model.forward(x_in).view(-1, len(ALPHABET) + CAESAR_OFFSET)\n",
        "        l = loss(out, y_in)\n",
        "        train_loss += l.item()\n",
        "        batch_acc = (out.argmax(dim=1) == y_in)\n",
        "        train_acc += batch_acc.sum().item() / batch_acc.shape[0]\n",
        "        l.backward()\n",
        "        optimizer.step()\n",
        "        iter_num += 1\n",
        "    print(\n",
        "        f\"Epoch: {epoch}, loss: {train_loss:.4f}, acc: \"\n",
        "        f\"{train_acc / iter_num:.4f}\",\n",
        "        end=\" | \"\n",
        "    )\n",
        "    test_loss, test_acc, iter_num = .0, .0, .0\n",
        "    model.eval()\n",
        "    for x_in, y_in in test_dl:\n",
        "        x_in = x_in\n",
        "        y_in = y_in.view(1, -1).squeeze()\n",
        "        out = model.forward(x_in).view(-1, len(ALPHABET) + CAESAR_OFFSET)\n",
        "        l = loss(out, y_in)\n",
        "        test_loss += l.item()\n",
        "        batch_acc = (out.argmax(dim=1) == y_in)\n",
        "        test_acc += batch_acc.sum().item() / batch_acc.shape[0]\n",
        "        iter_num += 1\n",
        "    print(\n",
        "        f\"test loss: {test_loss:.4f}, test acc: {test_acc / iter_num:.4f} | \"\n",
        "        f\"{time.time() - start_epoch_time:.2f} sec.\"\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D1V2HqD4mlNF",
        "outputId": "10e6aa1f-0876-404e-f3c2-d235209ce068"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, loss: 42.9307, acc: 0.3520 | test loss: 7.2557, test acc: 0.6192 | 1.66 sec.\n",
            "Epoch: 1, loss: 32.2231, acc: 0.7041 | test loss: 5.4864, test acc: 0.7992 | 1.06 sec.\n",
            "Epoch: 2, loss: 24.8675, acc: 0.7986 | test loss: 4.3036, test acc: 0.8195 | 0.72 sec.\n",
            "Epoch: 3, loss: 20.0077, acc: 0.8244 | test loss: 3.5226, test acc: 0.8535 | 0.70 sec.\n",
            "Epoch: 4, loss: 16.6377, acc: 0.8516 | test loss: 2.9727, test acc: 0.8648 | 0.70 sec.\n",
            "Epoch: 5, loss: 14.1686, acc: 0.8621 | test loss: 2.5365, test acc: 0.8702 | 0.71 sec.\n",
            "Epoch: 6, loss: 12.4016, acc: 0.8674 | test loss: 2.2163, test acc: 0.8790 | 0.70 sec.\n",
            "Epoch: 7, loss: 10.9149, acc: 0.8732 | test loss: 1.9979, test acc: 0.8842 | 0.70 sec.\n",
            "Epoch: 8, loss: 9.9612, acc: 0.8817 | test loss: 1.8038, test acc: 0.8988 | 0.74 sec.\n",
            "Epoch: 9, loss: 8.8690, acc: 0.8959 | test loss: 1.6813, test acc: 0.9043 | 0.71 sec.\n",
            "Epoch: 10, loss: 8.3495, acc: 0.9016 | test loss: 1.5533, test acc: 0.9130 | 0.70 sec.\n",
            "Epoch: 11, loss: 7.6448, acc: 0.9112 | test loss: 1.4423, test acc: 0.9182 | 0.70 sec.\n",
            "Epoch: 12, loss: 7.2881, acc: 0.9147 | test loss: 1.3696, test acc: 0.9188 | 0.70 sec.\n",
            "Epoch: 13, loss: 6.8228, acc: 0.9177 | test loss: 1.2499, test acc: 0.9293 | 0.71 sec.\n",
            "Epoch: 14, loss: 6.4867, acc: 0.9227 | test loss: 1.1765, test acc: 0.9353 | 0.70 sec.\n",
            "Epoch: 15, loss: 6.2295, acc: 0.9237 | test loss: 1.0795, test acc: 0.9383 | 0.72 sec.\n",
            "Epoch: 16, loss: 5.8909, acc: 0.9247 | test loss: 1.0992, test acc: 0.9363 | 0.71 sec.\n",
            "Epoch: 17, loss: 5.6005, acc: 0.9313 | test loss: 1.0129, test acc: 0.9437 | 0.70 sec.\n",
            "Epoch: 18, loss: 5.4732, acc: 0.9314 | test loss: 1.0055, test acc: 0.9392 | 0.72 sec.\n",
            "Epoch: 19, loss: 5.2163, acc: 0.9331 | test loss: 0.9925, test acc: 0.9393 | 1.09 sec.\n",
            "Epoch: 20, loss: 5.0769, acc: 0.9329 | test loss: 0.9007, test acc: 0.9422 | 1.07 sec.\n",
            "Epoch: 21, loss: 4.9131, acc: 0.9335 | test loss: 0.8743, test acc: 0.9438 | 0.73 sec.\n",
            "Epoch: 22, loss: 4.6753, acc: 0.9388 | test loss: 0.8649, test acc: 0.9468 | 0.72 sec.\n",
            "Epoch: 23, loss: 4.5910, acc: 0.9401 | test loss: 0.8009, test acc: 0.9515 | 0.70 sec.\n",
            "Epoch: 24, loss: 4.3817, acc: 0.9416 | test loss: 0.7556, test acc: 0.9562 | 0.70 sec.\n",
            "Epoch: 25, loss: 4.3370, acc: 0.9406 | test loss: 0.7721, test acc: 0.9518 | 0.71 sec.\n",
            "Epoch: 26, loss: 4.2089, acc: 0.9415 | test loss: 0.7345, test acc: 0.9545 | 0.71 sec.\n",
            "Epoch: 27, loss: 4.0080, acc: 0.9442 | test loss: 0.7271, test acc: 0.9538 | 0.72 sec.\n",
            "Epoch: 28, loss: 3.9913, acc: 0.9427 | test loss: 0.6637, test acc: 0.9585 | 0.72 sec.\n",
            "Epoch: 29, loss: 3.8696, acc: 0.9444 | test loss: 0.6734, test acc: 0.9563 | 0.71 sec.\n",
            "Epoch: 30, loss: 3.7383, acc: 0.9449 | test loss: 0.6522, test acc: 0.9585 | 0.71 sec.\n",
            "Epoch: 31, loss: 3.4948, acc: 0.9511 | test loss: 0.6315, test acc: 0.9603 | 0.72 sec.\n",
            "Epoch: 32, loss: 3.4718, acc: 0.9509 | test loss: 0.5929, test acc: 0.9630 | 0.72 sec.\n",
            "Epoch: 33, loss: 3.3265, acc: 0.9529 | test loss: 0.5330, test acc: 0.9672 | 0.70 sec.\n",
            "Epoch: 34, loss: 3.3973, acc: 0.9500 | test loss: 0.5395, test acc: 0.9660 | 0.71 sec.\n",
            "Epoch: 35, loss: 3.3480, acc: 0.9513 | test loss: 0.6163, test acc: 0.9583 | 0.70 sec.\n",
            "Epoch: 36, loss: 3.2453, acc: 0.9532 | test loss: 0.5488, test acc: 0.9653 | 0.70 sec.\n",
            "Epoch: 37, loss: 3.2539, acc: 0.9522 | test loss: 0.5374, test acc: 0.9655 | 0.71 sec.\n",
            "Epoch: 38, loss: 3.1631, acc: 0.9534 | test loss: 0.5501, test acc: 0.9642 | 0.71 sec.\n",
            "Epoch: 39, loss: 3.0854, acc: 0.9541 | test loss: 0.5519, test acc: 0.9632 | 0.71 sec.\n",
            "Epoch: 40, loss: 2.9856, acc: 0.9559 | test loss: 0.5000, test acc: 0.9677 | 0.70 sec.\n",
            "Epoch: 41, loss: 2.9980, acc: 0.9543 | test loss: 0.4863, test acc: 0.9670 | 0.71 sec.\n",
            "Epoch: 42, loss: 2.8862, acc: 0.9565 | test loss: 0.4643, test acc: 0.9687 | 0.71 sec.\n",
            "Epoch: 43, loss: 2.8603, acc: 0.9571 | test loss: 0.5134, test acc: 0.9635 | 0.72 sec.\n",
            "Epoch: 44, loss: 2.8076, acc: 0.9577 | test loss: 0.4632, test acc: 0.9682 | 0.71 sec.\n",
            "Epoch: 45, loss: 2.7727, acc: 0.9581 | test loss: 0.4958, test acc: 0.9638 | 0.72 sec.\n",
            "Epoch: 46, loss: 2.6394, acc: 0.9604 | test loss: 0.4567, test acc: 0.9678 | 0.71 sec.\n",
            "Epoch: 47, loss: 2.6710, acc: 0.9592 | test loss: 0.3956, test acc: 0.9740 | 0.71 sec.\n",
            "Epoch: 48, loss: 2.5450, acc: 0.9626 | test loss: 0.4036, test acc: 0.9733 | 0.71 sec.\n",
            "Epoch: 49, loss: 2.5960, acc: 0.9628 | test loss: 0.4232, test acc: 0.9722 | 0.71 sec.\n",
            "Epoch: 50, loss: 2.4424, acc: 0.9654 | test loss: 0.4357, test acc: 0.9720 | 0.71 sec.\n",
            "Epoch: 51, loss: 2.4970, acc: 0.9643 | test loss: 0.4000, test acc: 0.9737 | 0.72 sec.\n",
            "Epoch: 52, loss: 2.4273, acc: 0.9655 | test loss: 0.3893, test acc: 0.9753 | 0.70 sec.\n",
            "Epoch: 53, loss: 2.4475, acc: 0.9661 | test loss: 0.4136, test acc: 0.9728 | 0.71 sec.\n",
            "Epoch: 54, loss: 2.4111, acc: 0.9679 | test loss: 0.3605, test acc: 0.9795 | 0.71 sec.\n",
            "Epoch: 55, loss: 2.2655, acc: 0.9706 | test loss: 0.3993, test acc: 0.9758 | 0.71 sec.\n",
            "Epoch: 56, loss: 2.3183, acc: 0.9702 | test loss: 0.3915, test acc: 0.9757 | 0.71 sec.\n",
            "Epoch: 57, loss: 2.1897, acc: 0.9726 | test loss: 0.3875, test acc: 0.9780 | 0.86 sec.\n",
            "Epoch: 58, loss: 2.1687, acc: 0.9727 | test loss: 0.3873, test acc: 0.9778 | 0.86 sec.\n",
            "Epoch: 59, loss: 2.2469, acc: 0.9710 | test loss: 0.3723, test acc: 0.9788 | 0.86 sec.\n",
            "Epoch: 60, loss: 2.0017, acc: 0.9759 | test loss: 0.3503, test acc: 0.9800 | 0.72 sec.\n",
            "Epoch: 61, loss: 2.1434, acc: 0.9727 | test loss: 0.3650, test acc: 0.9797 | 0.70 sec.\n",
            "Epoch: 62, loss: 2.1377, acc: 0.9732 | test loss: 0.3370, test acc: 0.9813 | 0.71 sec.\n",
            "Epoch: 63, loss: 2.1131, acc: 0.9739 | test loss: 0.2926, test acc: 0.9843 | 0.71 sec.\n",
            "Epoch: 64, loss: 2.0955, acc: 0.9743 | test loss: 0.3582, test acc: 0.9785 | 0.70 sec.\n",
            "Epoch: 65, loss: 2.0086, acc: 0.9755 | test loss: 0.3415, test acc: 0.9800 | 0.70 sec.\n",
            "Epoch: 66, loss: 1.9787, acc: 0.9755 | test loss: 0.3128, test acc: 0.9808 | 0.71 sec.\n",
            "Epoch: 67, loss: 1.9879, acc: 0.9751 | test loss: 0.3380, test acc: 0.9795 | 0.70 sec.\n",
            "Epoch: 68, loss: 1.9819, acc: 0.9754 | test loss: 0.3198, test acc: 0.9798 | 0.70 sec.\n",
            "Epoch: 69, loss: 1.9345, acc: 0.9757 | test loss: 0.2884, test acc: 0.9830 | 0.71 sec.\n",
            "Epoch: 70, loss: 1.9265, acc: 0.9757 | test loss: 0.3384, test acc: 0.9805 | 0.72 sec.\n",
            "Epoch: 71, loss: 1.8741, acc: 0.9774 | test loss: 0.3085, test acc: 0.9830 | 0.72 sec.\n",
            "Epoch: 72, loss: 1.8895, acc: 0.9773 | test loss: 0.2601, test acc: 0.9865 | 0.72 sec.\n",
            "Epoch: 73, loss: 1.7966, acc: 0.9786 | test loss: 0.2690, test acc: 0.9857 | 0.70 sec.\n",
            "Epoch: 74, loss: 1.8645, acc: 0.9776 | test loss: 0.3176, test acc: 0.9827 | 0.70 sec.\n",
            "Epoch: 75, loss: 1.8393, acc: 0.9781 | test loss: 0.3052, test acc: 0.9838 | 0.70 sec.\n",
            "Epoch: 76, loss: 1.7674, acc: 0.9788 | test loss: 0.3189, test acc: 0.9818 | 0.71 sec.\n",
            "Epoch: 77, loss: 1.7376, acc: 0.9796 | test loss: 0.2602, test acc: 0.9862 | 0.71 sec.\n",
            "Epoch: 78, loss: 1.7639, acc: 0.9792 | test loss: 0.2808, test acc: 0.9852 | 0.70 sec.\n",
            "Epoch: 79, loss: 1.7467, acc: 0.9791 | test loss: 0.2935, test acc: 0.9838 | 0.73 sec.\n",
            "Epoch: 80, loss: 1.7112, acc: 0.9800 | test loss: 0.2835, test acc: 0.9842 | 0.71 sec.\n",
            "Epoch: 81, loss: 1.6272, acc: 0.9811 | test loss: 0.2099, test acc: 0.9900 | 0.69 sec.\n",
            "Epoch: 82, loss: 1.6847, acc: 0.9802 | test loss: 0.2766, test acc: 0.9845 | 0.71 sec.\n",
            "Epoch: 83, loss: 1.6306, acc: 0.9813 | test loss: 0.2624, test acc: 0.9860 | 0.71 sec.\n",
            "Epoch: 84, loss: 1.6154, acc: 0.9815 | test loss: 0.2412, test acc: 0.9882 | 0.71 sec.\n",
            "Epoch: 85, loss: 1.6056, acc: 0.9822 | test loss: 0.2699, test acc: 0.9865 | 0.71 sec.\n",
            "Epoch: 86, loss: 1.5754, acc: 0.9832 | test loss: 0.2437, test acc: 0.9882 | 0.72 sec.\n",
            "Epoch: 87, loss: 1.5355, acc: 0.9839 | test loss: 0.2491, test acc: 0.9878 | 0.71 sec.\n",
            "Epoch: 88, loss: 1.5152, acc: 0.9841 | test loss: 0.2417, test acc: 0.9887 | 0.71 sec.\n",
            "Epoch: 89, loss: 1.5310, acc: 0.9842 | test loss: 0.2440, test acc: 0.9893 | 0.71 sec.\n",
            "Epoch: 90, loss: 1.4258, acc: 0.9859 | test loss: 0.2432, test acc: 0.9895 | 0.71 sec.\n",
            "Epoch: 91, loss: 1.5271, acc: 0.9844 | test loss: 0.2713, test acc: 0.9878 | 0.70 sec.\n",
            "Epoch: 92, loss: 1.5114, acc: 0.9848 | test loss: 0.2223, test acc: 0.9908 | 0.70 sec.\n",
            "Epoch: 93, loss: 1.4294, acc: 0.9859 | test loss: 0.2482, test acc: 0.9900 | 0.71 sec.\n",
            "Epoch: 94, loss: 1.3959, acc: 0.9866 | test loss: 0.2045, test acc: 0.9918 | 0.70 sec.\n",
            "Epoch: 95, loss: 1.4429, acc: 0.9859 | test loss: 0.2140, test acc: 0.9920 | 0.72 sec.\n",
            "Epoch: 96, loss: 1.4339, acc: 0.9861 | test loss: 0.2104, test acc: 0.9923 | 0.72 sec.\n",
            "Epoch: 97, loss: 1.2997, acc: 0.9876 | test loss: 0.2115, test acc: 0.9915 | 0.71 sec.\n",
            "Epoch: 98, loss: 1.3965, acc: 0.9862 | test loss: 0.2261, test acc: 0.9907 | 0.72 sec.\n",
            "Epoch: 99, loss: 1.4175, acc: 0.9859 | test loss: 0.2107, test acc: 0.9913 | 0.71 sec.\n",
            "Epoch: 100, loss: 1.3610, acc: 0.9868 | test loss: 0.2194, test acc: 0.9925 | 0.70 sec.\n",
            "Epoch: 101, loss: 1.3958, acc: 0.9872 | test loss: 0.2109, test acc: 0.9923 | 0.73 sec.\n",
            "Epoch: 102, loss: 1.3549, acc: 0.9874 | test loss: 0.2062, test acc: 0.9935 | 0.72 sec.\n",
            "Epoch: 103, loss: 1.3504, acc: 0.9875 | test loss: 0.2236, test acc: 0.9923 | 0.71 sec.\n",
            "Epoch: 104, loss: 1.2436, acc: 0.9892 | test loss: 0.1971, test acc: 0.9932 | 0.71 sec.\n",
            "Epoch: 105, loss: 1.3171, acc: 0.9882 | test loss: 0.2102, test acc: 0.9923 | 0.70 sec.\n",
            "Epoch: 106, loss: 1.2392, acc: 0.9893 | test loss: 0.2137, test acc: 0.9918 | 0.71 sec.\n",
            "Epoch: 107, loss: 1.1720, acc: 0.9900 | test loss: 0.1737, test acc: 0.9935 | 0.72 sec.\n",
            "Epoch: 108, loss: 1.2985, acc: 0.9882 | test loss: 0.1692, test acc: 0.9947 | 0.70 sec.\n",
            "Epoch: 109, loss: 1.2825, acc: 0.9884 | test loss: 0.2060, test acc: 0.9930 | 0.72 sec.\n",
            "Epoch: 110, loss: 1.2519, acc: 0.9887 | test loss: 0.2048, test acc: 0.9927 | 0.72 sec.\n",
            "Epoch: 111, loss: 1.2649, acc: 0.9885 | test loss: 0.1962, test acc: 0.9922 | 0.70 sec.\n",
            "Epoch: 112, loss: 1.2104, acc: 0.9892 | test loss: 0.1793, test acc: 0.9935 | 0.71 sec.\n",
            "Epoch: 113, loss: 1.1759, acc: 0.9897 | test loss: 0.1655, test acc: 0.9945 | 0.71 sec.\n",
            "Epoch: 114, loss: 1.1698, acc: 0.9894 | test loss: 0.1761, test acc: 0.9937 | 0.71 sec.\n",
            "Epoch: 115, loss: 1.1803, acc: 0.9894 | test loss: 0.1856, test acc: 0.9932 | 0.71 sec.\n",
            "Epoch: 116, loss: 1.1785, acc: 0.9899 | test loss: 0.1917, test acc: 0.9930 | 0.71 sec.\n",
            "Epoch: 117, loss: 1.1277, acc: 0.9905 | test loss: 0.1605, test acc: 0.9945 | 0.70 sec.\n",
            "Epoch: 118, loss: 1.1139, acc: 0.9903 | test loss: 0.1821, test acc: 0.9937 | 0.70 sec.\n",
            "Epoch: 119, loss: 1.1506, acc: 0.9901 | test loss: 0.1904, test acc: 0.9928 | 0.72 sec.\n",
            "Epoch: 120, loss: 1.0649, acc: 0.9914 | test loss: 0.1537, test acc: 0.9950 | 0.72 sec.\n",
            "Epoch: 121, loss: 1.0700, acc: 0.9911 | test loss: 0.1876, test acc: 0.9928 | 0.72 sec.\n",
            "Epoch: 122, loss: 1.0779, acc: 0.9907 | test loss: 0.1679, test acc: 0.9935 | 0.70 sec.\n",
            "Epoch: 123, loss: 1.1158, acc: 0.9903 | test loss: 0.1585, test acc: 0.9945 | 0.72 sec.\n",
            "Epoch: 124, loss: 1.0710, acc: 0.9907 | test loss: 0.1773, test acc: 0.9937 | 0.72 sec.\n",
            "Epoch: 125, loss: 1.0332, acc: 0.9914 | test loss: 0.1775, test acc: 0.9933 | 0.71 sec.\n",
            "Epoch: 126, loss: 1.1014, acc: 0.9905 | test loss: 0.1407, test acc: 0.9953 | 0.71 sec.\n",
            "Epoch: 127, loss: 1.0921, acc: 0.9905 | test loss: 0.1674, test acc: 0.9940 | 0.71 sec.\n",
            "Epoch: 128, loss: 1.0562, acc: 0.9909 | test loss: 0.1715, test acc: 0.9937 | 0.70 sec.\n",
            "Epoch: 129, loss: 1.0431, acc: 0.9910 | test loss: 0.1623, test acc: 0.9938 | 0.70 sec.\n",
            "Epoch: 130, loss: 0.9972, acc: 0.9913 | test loss: 0.1332, test acc: 0.9950 | 0.73 sec.\n",
            "Epoch: 131, loss: 1.0562, acc: 0.9907 | test loss: 0.1657, test acc: 0.9940 | 0.71 sec.\n",
            "Epoch: 132, loss: 1.0426, acc: 0.9909 | test loss: 0.1703, test acc: 0.9932 | 0.71 sec.\n",
            "Epoch: 133, loss: 1.0118, acc: 0.9912 | test loss: 0.1784, test acc: 0.9927 | 0.73 sec.\n",
            "Epoch: 134, loss: 0.9720, acc: 0.9920 | test loss: 0.1769, test acc: 0.9930 | 0.71 sec.\n",
            "Epoch: 135, loss: 1.0261, acc: 0.9910 | test loss: 0.1433, test acc: 0.9943 | 0.70 sec.\n",
            "Epoch: 136, loss: 0.9808, acc: 0.9914 | test loss: 0.1386, test acc: 0.9950 | 0.71 sec.\n",
            "Epoch: 137, loss: 0.9701, acc: 0.9916 | test loss: 0.1546, test acc: 0.9942 | 0.71 sec.\n",
            "Epoch: 138, loss: 0.9997, acc: 0.9912 | test loss: 0.1498, test acc: 0.9937 | 0.71 sec.\n",
            "Epoch: 139, loss: 0.9552, acc: 0.9916 | test loss: 0.1575, test acc: 0.9935 | 0.72 sec.\n",
            "Epoch: 140, loss: 0.9168, acc: 0.9922 | test loss: 0.1242, test acc: 0.9952 | 0.72 sec.\n",
            "Epoch: 141, loss: 0.9297, acc: 0.9922 | test loss: 0.1472, test acc: 0.9948 | 0.71 sec.\n",
            "Epoch: 142, loss: 0.9554, acc: 0.9917 | test loss: 0.1321, test acc: 0.9957 | 0.70 sec.\n",
            "Epoch: 143, loss: 0.9538, acc: 0.9918 | test loss: 0.1585, test acc: 0.9943 | 0.71 sec.\n",
            "Epoch: 144, loss: 0.9474, acc: 0.9921 | test loss: 0.1560, test acc: 0.9937 | 0.73 sec.\n",
            "Epoch: 145, loss: 0.8948, acc: 0.9926 | test loss: 0.1450, test acc: 0.9948 | 0.71 sec.\n",
            "Epoch: 146, loss: 0.9090, acc: 0.9925 | test loss: 0.1431, test acc: 0.9945 | 0.71 sec.\n",
            "Epoch: 147, loss: 0.9145, acc: 0.9924 | test loss: 0.1265, test acc: 0.9950 | 0.71 sec.\n",
            "Epoch: 148, loss: 0.9194, acc: 0.9922 | test loss: 0.1011, test acc: 0.9968 | 0.71 sec.\n",
            "Epoch: 149, loss: 0.8796, acc: 0.9931 | test loss: 0.1385, test acc: 0.9947 | 0.71 sec.\n",
            "Epoch: 150, loss: 0.8908, acc: 0.9930 | test loss: 0.1342, test acc: 0.9960 | 0.70 sec.\n",
            "Epoch: 151, loss: 0.8808, acc: 0.9930 | test loss: 0.1314, test acc: 0.9958 | 0.71 sec.\n",
            "Epoch: 152, loss: 0.8667, acc: 0.9930 | test loss: 0.1247, test acc: 0.9955 | 0.68 sec.\n",
            "Epoch: 153, loss: 0.8238, acc: 0.9938 | test loss: 0.1401, test acc: 0.9948 | 0.72 sec.\n",
            "Epoch: 154, loss: 0.8794, acc: 0.9929 | test loss: 0.1230, test acc: 0.9955 | 0.71 sec.\n",
            "Epoch: 155, loss: 0.8445, acc: 0.9932 | test loss: 0.1265, test acc: 0.9952 | 0.70 sec.\n",
            "Epoch: 156, loss: 0.8660, acc: 0.9931 | test loss: 0.1284, test acc: 0.9957 | 0.71 sec.\n",
            "Epoch: 157, loss: 0.8554, acc: 0.9934 | test loss: 0.1334, test acc: 0.9955 | 0.72 sec.\n",
            "Epoch: 158, loss: 0.8239, acc: 0.9939 | test loss: 0.1158, test acc: 0.9960 | 0.71 sec.\n",
            "Epoch: 159, loss: 0.8365, acc: 0.9936 | test loss: 0.1255, test acc: 0.9957 | 0.72 sec.\n",
            "Epoch: 160, loss: 0.8470, acc: 0.9935 | test loss: 0.1335, test acc: 0.9948 | 0.72 sec.\n",
            "Epoch: 161, loss: 0.8015, acc: 0.9941 | test loss: 0.1343, test acc: 0.9947 | 0.72 sec.\n",
            "Epoch: 162, loss: 0.8254, acc: 0.9937 | test loss: 0.1379, test acc: 0.9947 | 0.71 sec.\n",
            "Epoch: 163, loss: 0.8201, acc: 0.9938 | test loss: 0.1183, test acc: 0.9960 | 0.73 sec.\n",
            "Epoch: 164, loss: 0.7840, acc: 0.9941 | test loss: 0.1306, test acc: 0.9952 | 0.72 sec.\n",
            "Epoch: 165, loss: 0.7809, acc: 0.9944 | test loss: 0.1300, test acc: 0.9948 | 0.70 sec.\n",
            "Epoch: 166, loss: 0.7850, acc: 0.9941 | test loss: 0.1225, test acc: 0.9952 | 0.70 sec.\n",
            "Epoch: 167, loss: 0.7863, acc: 0.9941 | test loss: 0.1188, test acc: 0.9950 | 0.71 sec.\n",
            "Epoch: 168, loss: 0.7768, acc: 0.9943 | test loss: 0.1039, test acc: 0.9967 | 0.70 sec.\n",
            "Epoch: 169, loss: 0.7658, acc: 0.9942 | test loss: 0.1082, test acc: 0.9967 | 0.73 sec.\n",
            "Epoch: 170, loss: 0.7792, acc: 0.9941 | test loss: 0.1082, test acc: 0.9960 | 0.74 sec.\n",
            "Epoch: 171, loss: 0.7784, acc: 0.9940 | test loss: 0.1252, test acc: 0.9948 | 0.71 sec.\n",
            "Epoch: 172, loss: 0.7342, acc: 0.9946 | test loss: 0.1023, test acc: 0.9967 | 0.72 sec.\n",
            "Epoch: 173, loss: 0.7531, acc: 0.9943 | test loss: 0.1106, test acc: 0.9962 | 0.71 sec.\n",
            "Epoch: 174, loss: 0.7492, acc: 0.9943 | test loss: 0.0995, test acc: 0.9965 | 0.71 sec.\n",
            "Epoch: 175, loss: 0.6510, acc: 0.9955 | test loss: 0.1244, test acc: 0.9953 | 0.71 sec.\n",
            "Epoch: 176, loss: 0.7489, acc: 0.9942 | test loss: 0.1133, test acc: 0.9953 | 0.72 sec.\n",
            "Epoch: 177, loss: 0.7437, acc: 0.9943 | test loss: 0.0914, test acc: 0.9967 | 0.70 sec.\n",
            "Epoch: 178, loss: 0.7217, acc: 0.9946 | test loss: 0.1145, test acc: 0.9957 | 0.70 sec.\n",
            "Epoch: 179, loss: 0.7069, acc: 0.9947 | test loss: 0.1046, test acc: 0.9960 | 0.70 sec.\n",
            "Epoch: 180, loss: 0.6867, acc: 0.9949 | test loss: 0.1155, test acc: 0.9958 | 0.73 sec.\n",
            "Epoch: 181, loss: 0.7283, acc: 0.9943 | test loss: 0.1230, test acc: 0.9953 | 0.71 sec.\n",
            "Epoch: 182, loss: 0.7213, acc: 0.9944 | test loss: 0.1223, test acc: 0.9955 | 0.72 sec.\n",
            "Epoch: 183, loss: 0.7090, acc: 0.9945 | test loss: 0.1023, test acc: 0.9965 | 0.72 sec.\n",
            "Epoch: 184, loss: 0.6853, acc: 0.9947 | test loss: 0.1250, test acc: 0.9948 | 0.71 sec.\n",
            "Epoch: 185, loss: 0.6684, acc: 0.9947 | test loss: 0.1109, test acc: 0.9957 | 0.71 sec.\n",
            "Epoch: 186, loss: 0.7046, acc: 0.9945 | test loss: 0.1151, test acc: 0.9950 | 0.71 sec.\n",
            "Epoch: 187, loss: 0.6780, acc: 0.9947 | test loss: 0.1072, test acc: 0.9958 | 0.72 sec.\n",
            "Epoch: 188, loss: 0.6628, acc: 0.9947 | test loss: 0.0951, test acc: 0.9967 | 0.72 sec.\n",
            "Epoch: 189, loss: 0.6919, acc: 0.9945 | test loss: 0.1015, test acc: 0.9955 | 0.71 sec.\n",
            "Epoch: 190, loss: 0.6500, acc: 0.9949 | test loss: 0.0914, test acc: 0.9965 | 0.72 sec.\n",
            "Epoch: 191, loss: 0.5942, acc: 0.9960 | test loss: 0.1099, test acc: 0.9953 | 0.72 sec.\n",
            "Epoch: 192, loss: 0.6773, acc: 0.9947 | test loss: 0.1112, test acc: 0.9960 | 0.70 sec.\n",
            "Epoch: 193, loss: 0.6688, acc: 0.9947 | test loss: 0.1108, test acc: 0.9957 | 0.71 sec.\n",
            "Epoch: 194, loss: 0.6623, acc: 0.9948 | test loss: 0.1024, test acc: 0.9950 | 0.70 sec.\n",
            "Epoch: 195, loss: 0.6666, acc: 0.9947 | test loss: 0.1049, test acc: 0.9957 | 0.71 sec.\n",
            "Epoch: 196, loss: 0.6662, acc: 0.9946 | test loss: 0.0807, test acc: 0.9972 | 0.71 sec.\n",
            "Epoch: 197, loss: 0.6475, acc: 0.9948 | test loss: 0.0954, test acc: 0.9965 | 0.71 sec.\n",
            "Epoch: 198, loss: 0.5942, acc: 0.9957 | test loss: 0.0957, test acc: 0.9962 | 0.71 sec.\n",
            "Epoch: 199, loss: 0.6513, acc: 0.9947 | test loss: 0.1000, test acc: 0.9958 | 0.70 sec.\n",
            "Epoch: 200, loss: 0.6166, acc: 0.9949 | test loss: 0.1025, test acc: 0.9962 | 0.72 sec.\n",
            "Epoch: 201, loss: 0.5956, acc: 0.9952 | test loss: 0.1015, test acc: 0.9955 | 0.71 sec.\n",
            "Epoch: 202, loss: 0.6321, acc: 0.9948 | test loss: 0.0941, test acc: 0.9965 | 0.72 sec.\n",
            "Epoch: 203, loss: 0.6084, acc: 0.9951 | test loss: 0.0723, test acc: 0.9978 | 0.71 sec.\n",
            "Epoch: 204, loss: 0.6240, acc: 0.9948 | test loss: 0.1008, test acc: 0.9963 | 0.72 sec.\n",
            "Epoch: 205, loss: 0.5754, acc: 0.9952 | test loss: 0.1059, test acc: 0.9955 | 0.71 sec.\n",
            "Epoch: 206, loss: 0.6073, acc: 0.9949 | test loss: 0.0954, test acc: 0.9957 | 0.71 sec.\n",
            "Epoch: 207, loss: 0.6116, acc: 0.9947 | test loss: 0.0924, test acc: 0.9963 | 0.71 sec.\n",
            "Epoch: 208, loss: 0.5809, acc: 0.9950 | test loss: 0.0793, test acc: 0.9972 | 0.70 sec.\n",
            "Epoch: 209, loss: 0.5781, acc: 0.9949 | test loss: 0.0814, test acc: 0.9972 | 0.70 sec.\n",
            "Epoch: 210, loss: 0.5831, acc: 0.9949 | test loss: 0.0913, test acc: 0.9963 | 0.71 sec.\n",
            "Epoch: 211, loss: 0.5647, acc: 0.9952 | test loss: 0.0941, test acc: 0.9960 | 0.71 sec.\n",
            "Epoch: 212, loss: 0.5807, acc: 0.9948 | test loss: 0.0904, test acc: 0.9963 | 0.71 sec.\n",
            "Epoch: 213, loss: 0.6007, acc: 0.9947 | test loss: 0.0660, test acc: 0.9972 | 0.71 sec.\n",
            "Epoch: 214, loss: 0.5835, acc: 0.9949 | test loss: 0.0782, test acc: 0.9965 | 0.71 sec.\n",
            "Epoch: 215, loss: 0.5391, acc: 0.9958 | test loss: 0.0875, test acc: 0.9957 | 0.70 sec.\n",
            "Epoch: 216, loss: 0.5684, acc: 0.9951 | test loss: 0.0868, test acc: 0.9965 | 0.71 sec.\n",
            "Epoch: 217, loss: 0.5767, acc: 0.9949 | test loss: 0.0667, test acc: 0.9968 | 0.72 sec.\n",
            "Epoch: 218, loss: 0.5643, acc: 0.9951 | test loss: 0.0884, test acc: 0.9957 | 0.72 sec.\n",
            "Epoch: 219, loss: 0.5694, acc: 0.9950 | test loss: 0.0824, test acc: 0.9970 | 0.70 sec.\n",
            "Epoch: 220, loss: 0.5649, acc: 0.9950 | test loss: 0.0883, test acc: 0.9962 | 0.70 sec.\n",
            "Epoch: 221, loss: 0.5544, acc: 0.9951 | test loss: 0.0889, test acc: 0.9953 | 0.70 sec.\n",
            "Epoch: 222, loss: 0.5074, acc: 0.9959 | test loss: 0.0683, test acc: 0.9973 | 0.70 sec.\n",
            "Epoch: 223, loss: 0.5578, acc: 0.9951 | test loss: 0.0954, test acc: 0.9958 | 0.72 sec.\n",
            "Epoch: 224, loss: 0.5501, acc: 0.9950 | test loss: 0.0801, test acc: 0.9965 | 0.71 sec.\n",
            "Epoch: 225, loss: 0.5559, acc: 0.9950 | test loss: 0.0753, test acc: 0.9970 | 0.70 sec.\n",
            "Epoch: 226, loss: 0.5548, acc: 0.9951 | test loss: 0.0722, test acc: 0.9970 | 0.70 sec.\n",
            "Epoch: 227, loss: 0.5449, acc: 0.9952 | test loss: 0.0719, test acc: 0.9973 | 0.72 sec.\n",
            "Epoch: 228, loss: 0.5177, acc: 0.9957 | test loss: 0.0745, test acc: 0.9975 | 0.70 sec.\n",
            "Epoch: 229, loss: 0.5149, acc: 0.9956 | test loss: 0.0849, test acc: 0.9965 | 0.70 sec.\n",
            "Epoch: 230, loss: 0.4887, acc: 0.9962 | test loss: 0.0787, test acc: 0.9970 | 0.71 sec.\n",
            "Epoch: 231, loss: 0.5451, acc: 0.9953 | test loss: 0.0686, test acc: 0.9972 | 0.70 sec.\n",
            "Epoch: 232, loss: 0.4883, acc: 0.9963 | test loss: 0.0821, test acc: 0.9963 | 0.72 sec.\n",
            "Epoch: 233, loss: 0.5301, acc: 0.9953 | test loss: 0.0837, test acc: 0.9965 | 0.70 sec.\n",
            "Epoch: 234, loss: 0.5162, acc: 0.9955 | test loss: 0.0788, test acc: 0.9968 | 0.71 sec.\n",
            "Epoch: 235, loss: 0.5084, acc: 0.9954 | test loss: 0.0835, test acc: 0.9963 | 0.70 sec.\n",
            "Epoch: 236, loss: 0.5264, acc: 0.9953 | test loss: 0.0706, test acc: 0.9972 | 0.71 sec.\n",
            "Epoch: 237, loss: 0.5126, acc: 0.9953 | test loss: 0.0913, test acc: 0.9962 | 0.71 sec.\n",
            "Epoch: 238, loss: 0.5172, acc: 0.9953 | test loss: 0.0843, test acc: 0.9965 | 0.70 sec.\n",
            "Epoch: 239, loss: 0.4801, acc: 0.9959 | test loss: 0.0716, test acc: 0.9972 | 0.72 sec.\n",
            "Epoch: 240, loss: 0.4928, acc: 0.9958 | test loss: 0.0891, test acc: 0.9968 | 0.72 sec.\n",
            "Epoch: 241, loss: 0.5163, acc: 0.9955 | test loss: 0.0705, test acc: 0.9972 | 0.72 sec.\n",
            "Epoch: 242, loss: 0.4983, acc: 0.9958 | test loss: 0.0752, test acc: 0.9972 | 0.71 sec.\n",
            "Epoch: 243, loss: 0.5107, acc: 0.9956 | test loss: 0.0808, test acc: 0.9970 | 0.71 sec.\n",
            "Epoch: 244, loss: 0.4840, acc: 0.9960 | test loss: 0.0808, test acc: 0.9970 | 0.72 sec.\n",
            "Epoch: 245, loss: 0.4994, acc: 0.9957 | test loss: 0.0710, test acc: 0.9977 | 0.71 sec.\n",
            "Epoch: 246, loss: 0.4967, acc: 0.9956 | test loss: 0.0767, test acc: 0.9973 | 0.72 sec.\n",
            "Epoch: 247, loss: 0.4902, acc: 0.9958 | test loss: 0.0702, test acc: 0.9972 | 0.74 sec.\n",
            "Epoch: 248, loss: 0.4932, acc: 0.9956 | test loss: 0.0768, test acc: 0.9970 | 0.71 sec.\n",
            "Epoch: 249, loss: 0.4855, acc: 0.9956 | test loss: 0.0710, test acc: 0.9975 | 0.72 sec.\n",
            "Epoch: 250, loss: 0.4877, acc: 0.9955 | test loss: 0.0819, test acc: 0.9967 | 0.71 sec.\n",
            "Epoch: 251, loss: 0.4440, acc: 0.9961 | test loss: 0.0757, test acc: 0.9967 | 0.72 sec.\n",
            "Epoch: 252, loss: 0.4764, acc: 0.9957 | test loss: 0.0580, test acc: 0.9980 | 0.71 sec.\n",
            "Epoch: 253, loss: 0.4788, acc: 0.9956 | test loss: 0.0749, test acc: 0.9967 | 0.73 sec.\n",
            "Epoch: 254, loss: 0.4790, acc: 0.9957 | test loss: 0.0708, test acc: 0.9973 | 0.70 sec.\n",
            "Epoch: 255, loss: 0.4745, acc: 0.9957 | test loss: 0.0689, test acc: 0.9975 | 0.70 sec.\n",
            "Epoch: 256, loss: 0.4732, acc: 0.9957 | test loss: 0.0733, test acc: 0.9973 | 0.71 sec.\n",
            "Epoch: 257, loss: 0.4582, acc: 0.9958 | test loss: 0.0697, test acc: 0.9972 | 0.70 sec.\n",
            "Epoch: 258, loss: 0.4679, acc: 0.9957 | test loss: 0.0756, test acc: 0.9970 | 0.71 sec.\n",
            "Epoch: 259, loss: 0.4707, acc: 0.9956 | test loss: 0.0676, test acc: 0.9970 | 0.70 sec.\n",
            "Epoch: 260, loss: 0.4510, acc: 0.9958 | test loss: 0.0624, test acc: 0.9977 | 0.71 sec.\n",
            "Epoch: 261, loss: 0.4735, acc: 0.9956 | test loss: 0.0696, test acc: 0.9970 | 0.73 sec.\n",
            "Epoch: 262, loss: 0.4657, acc: 0.9956 | test loss: 0.0685, test acc: 0.9972 | 0.71 sec.\n",
            "Epoch: 263, loss: 0.4538, acc: 0.9957 | test loss: 0.0755, test acc: 0.9970 | 0.72 sec.\n",
            "Epoch: 264, loss: 0.4540, acc: 0.9956 | test loss: 0.0559, test acc: 0.9978 | 0.72 sec.\n",
            "Epoch: 265, loss: 0.4544, acc: 0.9958 | test loss: 0.0542, test acc: 0.9978 | 0.71 sec.\n",
            "Epoch: 266, loss: 0.4590, acc: 0.9956 | test loss: 0.0745, test acc: 0.9968 | 0.71 sec.\n",
            "Epoch: 267, loss: 0.4349, acc: 0.9960 | test loss: 0.0678, test acc: 0.9973 | 0.71 sec.\n",
            "Epoch: 268, loss: 0.4480, acc: 0.9956 | test loss: 0.0588, test acc: 0.9975 | 0.71 sec.\n",
            "Epoch: 269, loss: 0.4504, acc: 0.9957 | test loss: 0.0700, test acc: 0.9973 | 0.71 sec.\n",
            "Epoch: 270, loss: 0.4338, acc: 0.9958 | test loss: 0.0561, test acc: 0.9982 | 0.72 sec.\n",
            "Epoch: 271, loss: 0.4299, acc: 0.9959 | test loss: 0.0741, test acc: 0.9967 | 0.71 sec.\n",
            "Epoch: 272, loss: 0.4462, acc: 0.9956 | test loss: 0.0666, test acc: 0.9970 | 0.71 sec.\n",
            "Epoch: 273, loss: 0.4481, acc: 0.9956 | test loss: 0.0609, test acc: 0.9975 | 0.71 sec.\n",
            "Epoch: 274, loss: 0.4402, acc: 0.9956 | test loss: 0.0663, test acc: 0.9972 | 0.71 sec.\n",
            "Epoch: 275, loss: 0.4358, acc: 0.9956 | test loss: 0.0635, test acc: 0.9973 | 0.70 sec.\n",
            "Epoch: 276, loss: 0.4404, acc: 0.9956 | test loss: 0.0717, test acc: 0.9968 | 0.71 sec.\n",
            "Epoch: 277, loss: 0.4339, acc: 0.9957 | test loss: 0.0637, test acc: 0.9972 | 0.71 sec.\n",
            "Epoch: 278, loss: 0.4241, acc: 0.9958 | test loss: 0.0711, test acc: 0.9970 | 0.71 sec.\n",
            "Epoch: 279, loss: 0.4152, acc: 0.9958 | test loss: 0.0583, test acc: 0.9973 | 0.92 sec.\n",
            "Epoch: 280, loss: 0.3669, acc: 0.9964 | test loss: 0.0582, test acc: 0.9978 | 1.20 sec.\n",
            "Epoch: 281, loss: 0.3903, acc: 0.9965 | test loss: 0.0620, test acc: 0.9972 | 0.71 sec.\n",
            "Epoch: 282, loss: 0.4275, acc: 0.9957 | test loss: 0.0535, test acc: 0.9977 | 0.69 sec.\n",
            "Epoch: 283, loss: 0.4056, acc: 0.9959 | test loss: 0.0757, test acc: 0.9963 | 0.71 sec.\n",
            "Epoch: 284, loss: 0.3905, acc: 0.9960 | test loss: 0.0628, test acc: 0.9972 | 0.69 sec.\n",
            "Epoch: 285, loss: 0.4230, acc: 0.9956 | test loss: 0.0491, test acc: 0.9980 | 0.71 sec.\n",
            "Epoch: 286, loss: 0.4084, acc: 0.9959 | test loss: 0.0633, test acc: 0.9970 | 0.70 sec.\n",
            "Epoch: 287, loss: 0.4160, acc: 0.9957 | test loss: 0.0620, test acc: 0.9973 | 0.71 sec.\n",
            "Epoch: 288, loss: 0.3755, acc: 0.9961 | test loss: 0.0446, test acc: 0.9982 | 0.70 sec.\n",
            "Epoch: 289, loss: 0.4053, acc: 0.9958 | test loss: 0.0548, test acc: 0.9975 | 0.70 sec.\n",
            "Epoch: 290, loss: 0.4169, acc: 0.9956 | test loss: 0.0551, test acc: 0.9975 | 0.71 sec.\n",
            "Epoch: 291, loss: 0.3922, acc: 0.9959 | test loss: 0.0459, test acc: 0.9978 | 0.71 sec.\n",
            "Epoch: 292, loss: 0.4065, acc: 0.9957 | test loss: 0.0574, test acc: 0.9975 | 0.70 sec.\n",
            "Epoch: 293, loss: 0.4035, acc: 0.9957 | test loss: 0.0645, test acc: 0.9970 | 0.69 sec.\n",
            "Epoch: 294, loss: 0.3922, acc: 0.9959 | test loss: 0.0586, test acc: 0.9972 | 0.70 sec.\n",
            "Epoch: 295, loss: 0.4077, acc: 0.9957 | test loss: 0.0549, test acc: 0.9973 | 0.72 sec.\n",
            "Epoch: 296, loss: 0.3653, acc: 0.9966 | test loss: 0.0548, test acc: 0.9973 | 0.71 sec.\n",
            "Epoch: 297, loss: 0.3759, acc: 0.9961 | test loss: 0.0552, test acc: 0.9973 | 0.71 sec.\n",
            "Epoch: 298, loss: 0.3997, acc: 0.9957 | test loss: 0.0492, test acc: 0.9980 | 0.71 sec.\n",
            "Epoch: 299, loss: 0.3963, acc: 0.9958 | test loss: 0.0504, test acc: 0.9980 | 0.71 sec.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Проверим на произвольном батче\n",
        "idx = 5\n",
        "val_results = model(X_val.to(DEVICE)).argmax(dim=2)\n",
        "val_acc = (val_results == Y_val.to(DEVICE)).flatten()\n",
        "val_acc = (val_acc.sum() / val_acc.shape[0]).item()\n",
        "out_sentence = \"\".join([ALPHABET[i.item()] for i in val_results[idx]])\n",
        "true_sentence = \"\".join([ALPHABET[i.item()] for i in Y_val[idx]])\n",
        "\n",
        "sentence_idx = [ALPHABET[i] for i in true_sentence]\n",
        "encrypted_sentence_idx = [i + CAESAR_OFFSET for i in sentence_idx]\n",
        "encrypted_sentence = \"\".join([ALPHABET[i] for i in encrypted_sentence_idx])\n",
        "\n",
        "print(f\"Качество предсказаний : {val_acc:.4f}\")\n",
        "print(\"-\" * 50)\n",
        "print(f\"Зашифрованный текст: \\\"{encrypted_sentence}\\\"\")\n",
        "print(\"-\" * 50)\n",
        "print(f\"Расшифрованный текст: \\\"{out_sentence}\\\"\")\n",
        "print(\"-\" * 50)\n",
        "print(f\"Эталонный (фактический) текст:       \\\"{true_sentence}\\\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HPT4gGEwmaVo",
        "outputId": "80be8ad5-cd01-4181-8875-10ff978cb0b7"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Качество предсказаний : 0.9976\n",
            "--------------------------------------------------\n",
            "Зашифрованный текст: \"эПО.кЗЗу.йаАшОр О тА ми ОА5йПйчА О.жкЗЗьПСр мПйшОн9.Аря.Ср:Пмчм.юkХА2ApvdengЧdevcpЧ?esTsп»кЗЗУСИО  ОдАнмат мр ОАОА5ПСнкЗЗ,Арф.АраяшСйАрмнрй.А й5ПСн»кккttъккЗЗц9 тА.мP мАчйат 9.Ашйамнйдм.кЗЗ\tАчя.С тАмАдПСрйА ми й\n",
            "ГкЗЗьАшй.яА:йр5амч мАр5мПО тАрАнйдм.qкЗЗл:9шС\n",
            "Ачйр5м А.йPАа]чй\n",
            "»кЗЗ, мПм\n",
            "А—СчСйнжА.м\n",
            "Аен\"\n",
            "--------------------------------------------------\n",
            "Расшифрованный текст: \"Грим\n",
            "\t\tСмел чистить ногти перед ним,\n",
            "\t\tКрасноречивым сумасбродом[р] (#litres_trial_promo).\n",
            "\t\tЗащитник вольности и прав\n",
            "\t\tВ сём случае совсем неправ.\n",
            "\n",
            "\n",
            "XXV\n",
            "\n",
            "\t\tБыть можно дельным человеком\n",
            "\t\tИ думать о красе ногтей:\n",
            "\t\tК чему бесплодно спорить с веком?\n",
            "\t\tОбычай деспот меж людей.\n",
            "\t\tВторой Чадаев, мой Ев\"\n",
            "--------------------------------------------------\n",
            "Эталонный (фактический) текст:       \"Грим\n",
            "\t\tСмел чистить ногти перед ним,\n",
            "\t\tКрасноречивым сумасбродом[6] (#litres_trial_promo).\n",
            "\t\tЗащитник вольности и прав\n",
            "\t\tВ сём случае совсем неправ.\n",
            "\n",
            "\n",
            "XXV\n",
            "\n",
            "\t\tБыть можно дельным человеком\n",
            "\t\tИ думать о красе ногтей:\n",
            "\t\tК чему бесплодно спорить с веком?\n",
            "\t\tОбычай деспот меж людей.\n",
            "\t\tВторой Чадаев, мой Ев\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# сделаем обратную проверку через encrypted_sentence в deencrypted_sentence, вручную подставив в sentence_to_test значение True_sentence\n",
        "sentence_to_test = \"\"\"Грим\n",
        "\t\tСмел чистить ногти перед ним,\n",
        "\t\tКрасноречивым сумасбродом[6] (#litres_trial_promo).\n",
        "\t\tЗащитник вольности и прав\n",
        "\t\tВ сём случае совсем неправ.\n",
        "\n",
        "\n",
        "XXV\n",
        "\n",
        "\t\tБыть можно дельным человеком\n",
        "\t\tИ думать о красе ногтей:\n",
        "\t\tК чему бесплодно спорить с веком?\n",
        "\t\tОбычай деспот меж людей.\n",
        "\t\tВторой Чадаев, мой Ев\"\"\"\n",
        "sentence_idx = [ALPHABET[i] for i in sentence_to_test]\n",
        "encrypted_sentence_idx = [i + CAESAR_OFFSET for i in sentence_idx]\n",
        "encrypted_sentence = \"\".join([ALPHABET[i] for i in encrypted_sentence_idx])\n",
        "result = model(torch.tensor([encrypted_sentence_idx]).to(DEVICE)).argmax(dim=2)\n",
        "deencrypted_sentence = \"\".join([ALPHABET[i.item()] for i in result.flatten()])\n",
        "print(f\"Encrypted sentence is : \\\"{encrypted_sentence}\\\"\")\n",
        "print(\"-\" * 50)\n",
        "print(f\"De-encrypted sentence is : \\\"{deencrypted_sentence}\\\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v5W9xpV6ovtF",
        "outputId": "25cc9ef8-1aee-4ea4-8414-80f6d6285b23"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encrypted sentence is : \"эПО.кЗЗу.йаАшОр О тА ми ОА5йПйчА О.жкЗЗьПСр мПйшОн9.Аря.Ср:Пмчм.юkХА2ApvdengЧdevcpЧ?esTsп»кЗЗУСИО  ОдАнмат мр ОАОА5ПСнкЗЗ,Арф.АраяшСйАрмнрй.А й5ПСн»кккttъккЗЗц9 тА.мP мАчйат 9.Ашйамнйдм.кЗЗ\tАчя.С тАмАдПСрйА ми й\n",
            "ГкЗЗьАшй.яА:йр5амч мАр5мПО тАрАнйдм.qкЗЗл:9шС\n",
            "Ачйр5м А.йPАа]чй\n",
            "»кЗЗ, мПм\n",
            "А—СчСйнжА.м\n",
            "Аен\"\n",
            "--------------------------------------------------\n",
            "De-encrypted sentence is : \"Грим\n",
            "\t\tСмел чистить ногти перед ним,\n",
            "\t\tКрасноречивым сумасбродом[р] (#litres_trial_promo).\n",
            "\t\tЗащитник вольности и прав\n",
            "\t\tВ сём случае совсем неправ.\n",
            "\n",
            "\n",
            "XXV\n",
            "\n",
            "\t\tБыть можно дельным человеком\n",
            "\t\tИ думать о красе ногтей:\n",
            "\t\tК чему бесплодно спорить с веком?\n",
            "\t\tОбычай деспот меж людей.\n",
            "\t\tВторой Чадаев, мой Ев\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Как видим, используя для обучения архитектуру RNN, а также \"расширенный\" алфавит, составленный из всех символов корпуса,  удалось достичь очень высокой точности (99.76%) расшифровки текста, предварительно зашифрованного шифром Цезаря с фиксированным смещением (3 символа)."
      ],
      "metadata": {
        "id": "DToqPdl3lyr3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Задание 2. Выполните практическую работу из лекционного ноутбука:\n",
        "\n",
        "- Постройте RNN-ячейку на основе полносвязных слоёв\n",
        "- Примените построенную ячейку для генерации текста с выражениями героев сериала \"Симпсоны\"."
      ],
      "metadata": {
        "id": "optqw5vWlSUc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# загрузим данные\n",
        "df = pd.read_csv('data.csv')\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 652
        },
        "id": "am5yPrZWpds7",
        "outputId": "7edcbfea-5ebe-41d7-a9b0-35d05283c73a"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Unnamed: 0     id  episode_id  number  \\\n",
              "0           0  10368          35      29   \n",
              "1           1  10369          35      30   \n",
              "2           2  10370          35      31   \n",
              "3           3  10372          35      33   \n",
              "4           4  10374          35      35   \n",
              "\n",
              "                                            raw_text  timestamp_in_ms  \\\n",
              "0           Lisa Simpson: Maggie, look. What's that?           235000   \n",
              "1                    Lisa Simpson: Lee-mur. Lee-mur.           237000   \n",
              "2                    Lisa Simpson: Zee-boo. Zee-boo.           239000   \n",
              "3  Lisa Simpson: I'm trying to teach Maggie that ...           245000   \n",
              "4  Lisa Simpson: It's like an ox, only it has a h...           254000   \n",
              "\n",
              "   speaking_line  character_id  location_id raw_character_text  \\\n",
              "0           True             9          5.0       Lisa Simpson   \n",
              "1           True             9          5.0       Lisa Simpson   \n",
              "2           True             9          5.0       Lisa Simpson   \n",
              "3           True             9          5.0       Lisa Simpson   \n",
              "4           True             9          5.0       Lisa Simpson   \n",
              "\n",
              "  raw_location_text                                       spoken_words  \\\n",
              "0      Simpson Home                         Maggie, look. What's that?   \n",
              "1      Simpson Home                                  Lee-mur. Lee-mur.   \n",
              "2      Simpson Home                                  Zee-boo. Zee-boo.   \n",
              "3      Simpson Home  I'm trying to teach Maggie that nature doesn't...   \n",
              "4      Simpson Home  It's like an ox, only it has a hump and a dewl...   \n",
              "\n",
              "                                     normalized_text  word_count  \n",
              "0                             maggie look whats that         4.0  \n",
              "1                                    lee-mur lee-mur         2.0  \n",
              "2                                    zee-boo zee-boo         2.0  \n",
              "3  im trying to teach maggie that nature doesnt e...        24.0  \n",
              "4  its like an ox only it has a hump and a dewlap...        18.0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ceff904d-04bd-4cd8-8e60-c87065675b9d\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>id</th>\n",
              "      <th>episode_id</th>\n",
              "      <th>number</th>\n",
              "      <th>raw_text</th>\n",
              "      <th>timestamp_in_ms</th>\n",
              "      <th>speaking_line</th>\n",
              "      <th>character_id</th>\n",
              "      <th>location_id</th>\n",
              "      <th>raw_character_text</th>\n",
              "      <th>raw_location_text</th>\n",
              "      <th>spoken_words</th>\n",
              "      <th>normalized_text</th>\n",
              "      <th>word_count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>10368</td>\n",
              "      <td>35</td>\n",
              "      <td>29</td>\n",
              "      <td>Lisa Simpson: Maggie, look. What's that?</td>\n",
              "      <td>235000</td>\n",
              "      <td>True</td>\n",
              "      <td>9</td>\n",
              "      <td>5.0</td>\n",
              "      <td>Lisa Simpson</td>\n",
              "      <td>Simpson Home</td>\n",
              "      <td>Maggie, look. What's that?</td>\n",
              "      <td>maggie look whats that</td>\n",
              "      <td>4.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>10369</td>\n",
              "      <td>35</td>\n",
              "      <td>30</td>\n",
              "      <td>Lisa Simpson: Lee-mur. Lee-mur.</td>\n",
              "      <td>237000</td>\n",
              "      <td>True</td>\n",
              "      <td>9</td>\n",
              "      <td>5.0</td>\n",
              "      <td>Lisa Simpson</td>\n",
              "      <td>Simpson Home</td>\n",
              "      <td>Lee-mur. Lee-mur.</td>\n",
              "      <td>lee-mur lee-mur</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>10370</td>\n",
              "      <td>35</td>\n",
              "      <td>31</td>\n",
              "      <td>Lisa Simpson: Zee-boo. Zee-boo.</td>\n",
              "      <td>239000</td>\n",
              "      <td>True</td>\n",
              "      <td>9</td>\n",
              "      <td>5.0</td>\n",
              "      <td>Lisa Simpson</td>\n",
              "      <td>Simpson Home</td>\n",
              "      <td>Zee-boo. Zee-boo.</td>\n",
              "      <td>zee-boo zee-boo</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>10372</td>\n",
              "      <td>35</td>\n",
              "      <td>33</td>\n",
              "      <td>Lisa Simpson: I'm trying to teach Maggie that ...</td>\n",
              "      <td>245000</td>\n",
              "      <td>True</td>\n",
              "      <td>9</td>\n",
              "      <td>5.0</td>\n",
              "      <td>Lisa Simpson</td>\n",
              "      <td>Simpson Home</td>\n",
              "      <td>I'm trying to teach Maggie that nature doesn't...</td>\n",
              "      <td>im trying to teach maggie that nature doesnt e...</td>\n",
              "      <td>24.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>10374</td>\n",
              "      <td>35</td>\n",
              "      <td>35</td>\n",
              "      <td>Lisa Simpson: It's like an ox, only it has a h...</td>\n",
              "      <td>254000</td>\n",
              "      <td>True</td>\n",
              "      <td>9</td>\n",
              "      <td>5.0</td>\n",
              "      <td>Lisa Simpson</td>\n",
              "      <td>Simpson Home</td>\n",
              "      <td>It's like an ox, only it has a hump and a dewl...</td>\n",
              "      <td>its like an ox only it has a hump and a dewlap...</td>\n",
              "      <td>18.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ceff904d-04bd-4cd8-8e60-c87065675b9d')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-ceff904d-04bd-4cd8-8e60-c87065675b9d button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-ceff904d-04bd-4cd8-8e60-c87065675b9d');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "phrases = df['normalized_text'].tolist()\n",
        "phrases[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eLnteoSlqHYC",
        "outputId": "c5d4795a-7466-49cb-b33f-0569c6cdb149"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['maggie look whats that',\n",
              " 'lee-mur lee-mur',\n",
              " 'zee-boo zee-boo',\n",
              " 'im trying to teach maggie that nature doesnt end with the barnyard i want her to have all the advantages that i didnt have',\n",
              " 'its like an ox only it has a hump and a dewlap hump and dew-lap hump and dew-lap',\n",
              " 'you know his blood type how romantic',\n",
              " 'oh yeah whats my shoe size',\n",
              " 'ring',\n",
              " 'yes dad',\n",
              " 'ooh look maggie what is that do-dec-ah-edron dodecahedron']"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = [[c for c in ph] for ph in phrases if type(ph) is str]"
      ],
      "metadata": {
        "id": "DPZfMEgwqLRJ"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5LHT0RW4LvDJ"
      },
      "source": [
        "## Делаем массив с данными"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "CHARS = set('abcdefghijklmnopqrstuvwxyz ')\n",
        "INDEX_TO_CHAR = ['none'] + [w for w in CHARS]\n",
        "CHAR_TO_INDEX = {w: i for i, w in enumerate(INDEX_TO_CHAR)}"
      ],
      "metadata": {
        "id": "UIEGVdB7sZpE"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LEN = 50\n",
        "X = torch.zeros((len(text), MAX_LEN), dtype=int)\n",
        "for i in range(len(text)):\n",
        "    for j, w in enumerate(text[i]):\n",
        "        if j >= MAX_LEN:\n",
        "            break\n",
        "        X[i, j] = CHAR_TO_INDEX.get(w, CHAR_TO_INDEX['none'])\n",
        "\n",
        "X.to(DEVICE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hTa-eUFOsc1K",
        "outputId": "71e27831-d017-463f-9e38-b20eb7f9c361"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[20, 16,  5,  ...,  0,  0,  0],\n",
              "        [12,  3,  3,  ...,  0,  0,  0],\n",
              "        [25,  3,  3,  ...,  0,  0,  0],\n",
              "        ...,\n",
              "        [11, 10, 21,  ...,  0,  0,  0],\n",
              "        [20, 16,  7,  ..., 27, 12,  3],\n",
              "        [16, 11, 27,  ...,  0,  0,  0]])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X[0:3]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EpxiXWzfsgJC",
        "outputId": "68a33b2d-3420-4a9f-d676-e51d694c967d"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[20, 16,  5,  5,  2,  3, 27, 12, 10, 10, 23, 27, 21, 11, 16, 14, 26, 27,\n",
              "         14, 11, 16, 14,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
              "        [12,  3,  3,  0, 20,  8, 24, 27, 12,  3,  3,  0, 20,  8, 24,  0,  0,  0,\n",
              "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
              "        [25,  3,  3,  0,  9, 10, 10, 27, 25,  3,  3,  0,  9, 10, 10,  0,  0,  0,\n",
              "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85dMs3gbLvDO"
      },
      "source": [
        "## Смотрим на Embedding и RNN ячейку"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = torch.nn.Embedding(len(INDEX_TO_CHAR), 28)\n",
        "t = embeddings(X[0:3])\n",
        "t"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EHDLfKiGsgF6",
        "outputId": "90d59aa4-3b04-4ffa-d1d3-13e6b0ee7d50"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 0.7476,  1.8638, -0.5268,  ...,  0.6672, -0.7020,  0.2717],\n",
              "         [ 1.6652,  0.2178, -0.2961,  ..., -1.4403,  1.1554, -0.8749],\n",
              "         [ 1.7135, -1.8144,  0.3340,  ...,  0.9383, -0.3564, -0.5927],\n",
              "         ...,\n",
              "         [ 0.7903, -0.3701,  1.7428,  ...,  0.2809, -0.0804, -1.6894],\n",
              "         [ 0.7903, -0.3701,  1.7428,  ...,  0.2809, -0.0804, -1.6894],\n",
              "         [ 0.7903, -0.3701,  1.7428,  ...,  0.2809, -0.0804, -1.6894]],\n",
              "\n",
              "        [[-0.5318,  0.6047, -0.9767,  ...,  2.1632, -1.0145, -0.2269],\n",
              "         [ 0.3482, -1.0890,  1.3401,  ...,  0.0245, -0.5866, -0.9971],\n",
              "         [ 0.3482, -1.0890,  1.3401,  ...,  0.0245, -0.5866, -0.9971],\n",
              "         ...,\n",
              "         [ 0.7903, -0.3701,  1.7428,  ...,  0.2809, -0.0804, -1.6894],\n",
              "         [ 0.7903, -0.3701,  1.7428,  ...,  0.2809, -0.0804, -1.6894],\n",
              "         [ 0.7903, -0.3701,  1.7428,  ...,  0.2809, -0.0804, -1.6894]],\n",
              "\n",
              "        [[-0.7473,  0.7941,  0.6742,  ...,  0.5950, -0.6790,  0.8056],\n",
              "         [ 0.3482, -1.0890,  1.3401,  ...,  0.0245, -0.5866, -0.9971],\n",
              "         [ 0.3482, -1.0890,  1.3401,  ...,  0.0245, -0.5866, -0.9971],\n",
              "         ...,\n",
              "         [ 0.7903, -0.3701,  1.7428,  ...,  0.2809, -0.0804, -1.6894],\n",
              "         [ 0.7903, -0.3701,  1.7428,  ...,  0.2809, -0.0804, -1.6894],\n",
              "         [ 0.7903, -0.3701,  1.7428,  ...,  0.2809, -0.0804, -1.6894]]],\n",
              "       grad_fn=<EmbeddingBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "t.shape, X[0:3].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LUELuX1Zsq8v",
        "outputId": "77841210-b4fe-402a-aedc-f6daeff92c90"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([3, 50, 28]), torch.Size([3, 50]))"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rnn = torch.nn.RNN(28, 128, batch_first=True)\n",
        "o, s = rnn(t)\n",
        "o.shape, s.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4tckXHOetH9p",
        "outputId": "32e1c995-63c9-46dd-ca23-885f99d705e2"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([3, 50, 128]), torch.Size([1, 3, 128]))"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "o, s2 = rnn(t, s)\n",
        "o.shape, s2.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u0l5YU30tLge",
        "outputId": "5660cc9a-b508-41c3-ac3e-6fddd3de588b"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([3, 50, 128]), torch.Size([1, 3, 128]))"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3 слоя - embedding (28), скрытая ячейка (128), полносвязанный из состояния rnn в букву (28)\n",
        "class Network(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.embed = torch.nn.Embedding(28, 128)\n",
        "        self.rnn = torch.nn.RNN(128, 128, batch_first=True)\n",
        "        self.linear = torch.nn.Linear(128, 28)\n",
        "        \n",
        "    def forward(self, sentence, state=None):\n",
        "        embed = self.embed(sentence)\n",
        "        o, h = self.rnn(embed)\n",
        "        return self.linear(o)"
      ],
      "metadata": {
        "id": "ICj5_IzStOcd"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_EPOCHS = 200\n",
        "LEARNING_RATE = 0.01"
      ],
      "metadata": {
        "id": "CLUoQW4Py5Qz"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## выведем активное устройство для обучения модели\n",
        "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print (DEVICE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b2GXmIZ_ya7h",
        "outputId": "c09a72ff-f9a3-4c11-aa64-a18f5c950324"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = Network().to(DEVICE)"
      ],
      "metadata": {
        "id": "R9XqHTAftOZD"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = torch.nn.CrossEntropyLoss().to(DEVICE)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)"
      ],
      "metadata": {
        "id": "A2QnvVwftyLr"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-03-12T15:28:40.363097Z",
          "start_time": "2020-03-12T15:28:40.357998Z"
        },
        "id": "zck0cxpiLvDV"
      },
      "source": [
        "## Практика. Реализуйте код генерации следующей буквы на основе модели\n",
        "Логика такая:\n",
        "    - Сначала кормим в нее буквы из sentence (прогревая состояние)\n",
        "    - Затем пока не получим none (0) берем самую вероятную букву и добавляем ее в sentence\n",
        "    - Повторяем"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_sentence():\n",
        "    sentence = ['h', 'e', 'l', 'l', 'o']\n",
        "    x = torch.zeros((1, len(sentence)), dtype=int)\n",
        "    \n",
        "    for j, w in enumerate(sentence):\n",
        "        if j >= MAX_LEN:\n",
        "            break\n",
        "        \n",
        "        else:\n",
        "            x[0, j] = CHAR_TO_INDEX.get(w, CHAR_TO_INDEX['none'])\n",
        "    \n",
        "    for i in range(MAX_LEN):\n",
        "        o = model(x)\n",
        "        w = torch.argmax(o[-1, -1, :], keepdim=True)\n",
        "        x = torch.cat([x, w.unsqueeze(0)], axis=1)\n",
        "        ww = INDEX_TO_CHAR[w]\n",
        "        \n",
        "        if ww == 'none':\n",
        "            break\n",
        "            \n",
        "        sentence.append(ww)\n",
        "    \n",
        "    print(' '.join(sentence))"
      ],
      "metadata": {
        "id": "KJcoqug7uBj7"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_sentence()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g1vucCqouCPT",
        "outputId": "cb00db87-90b7-4324-8805-b78375a9cf71"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "h e l l o\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for ep in range(NUM_EPOCHS):\n",
        "    start = time.time()\n",
        "    train_loss = 0.\n",
        "    train_passed = 0\n",
        "\n",
        "    for i in range(int(len(X) / 100)):\n",
        "        batch = X[i * 100:(i + 1) * 100]\n",
        "        X_batch = batch[:, :-1]\n",
        "        Y_batch = batch[:, 1:].flatten()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        answers = model.forward(X_batch)\n",
        "        answers = answers.view(-1, len(INDEX_TO_CHAR))\n",
        "        loss = criterion(answers, Y_batch)\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_passed += 1\n",
        "\n",
        "    print(\"Epoch {}. Time: {:.3f}, Train loss: {:.3f}\".format(ep, time.time() - start, train_loss / train_passed))\n",
        "    generate_sentence()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c6nKTb3DuFAj",
        "outputId": "6a61f1e7-e68f-447e-a0c0-046654dfb49f"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0. Time: 5.508, Train loss: 1.390\n",
            "h e l l o   t h e   s e e   s t a r t   t h e   s e e   s t a r t   t h e   s e e   s t a r t   t h e   s e e\n",
            "Epoch 1. Time: 5.441, Train loss: 1.179\n",
            "h e l l o w   a   s a t e   a   s a t e   a   s a t e   a   s a t e   a   s a t e   a   s a t e   a   s a t e\n",
            "Epoch 2. Time: 5.533, Train loss: 1.130\n",
            "h e l l o   t h e   s e e   t h e   s e e   t h e   s e e   t h e   s e e   t h e   s e e   t h e   s e e   t\n",
            "Epoch 3. Time: 5.519, Train loss: 1.105\n",
            "h e l l o   t h e   s e e   t h e   s e e   t h e   s e e   t h e   s e e   t h e   s e e   t h e   s e e   t\n",
            "Epoch 4. Time: 5.536, Train loss: 1.089\n",
            "h e l l o   t h e   s p e a t   t h e   s p e a t   t h e   s p e a t   t h e   s p e a t   t h e   s p e a t\n",
            "Epoch 5. Time: 5.556, Train loss: 1.078\n",
            "h e l l o   i t   s t a r t   t h e   s c h o o l   m a k e   t o   s e e   y o u   s e e   t h e   s c h o o\n",
            "Epoch 6. Time: 5.546, Train loss: 1.069\n",
            "h e l l o   i t s   a   l i t t l e   t o   s e e m   t h e   s p r i n g f i e l d   t h e   s p r i n g f i\n",
            "Epoch 7. Time: 5.436, Train loss: 1.063\n",
            "h e l l o   t h e   s c r a c e   i   d o n t   k n o w   i   d o n t   k n o w   i   d o n t   k n o w   i  \n",
            "Epoch 8. Time: 5.466, Train loss: 1.057\n",
            "h e l l o   i   c a n t   s e e m s   a n d   t h e   s p r i n g f i e l d   t h e   s p r i n g f i e l d  \n",
            "Epoch 9. Time: 5.523, Train loss: 1.052\n",
            "h e l l o   i   t h i n k   i   t h i n k   i   t h i n k   i   t h i n k   i   t h i n k   i   t h i n k   i\n",
            "Epoch 10. Time: 5.487, Train loss: 1.049\n",
            "h e l l o   i t s   a   l i t t l e   t h e   s c h o o l   a r e   y o u   t h i n k   y o u   d o n t   w a\n",
            "Epoch 11. Time: 5.467, Train loss: 1.047\n",
            "h e l l o   i t   i s   a   l i t t l e   t o   s h e s   a   l i t t l e   t o   s h e s   a   l i t t l e  \n",
            "Epoch 12. Time: 5.466, Train loss: 1.044\n",
            "h e l l o   t h e   s t a r t   t h e   s t a r t   t h e   s t a r t   t h e   s t a r t   t h e   s t a r t\n",
            "Epoch 13. Time: 5.497, Train loss: 1.041\n",
            "h e l l o   i   g o t   t o   g e t   t o   g e t   t o   g e t   t o   g e t   t o   g e t   t o   g e t   t\n",
            "Epoch 14. Time: 5.496, Train loss: 1.041\n",
            "h e l l o w   t h e   s t u m a n   i   t h i n k   y o u   d o n t   k n o w   i   d o n t   k n o w   i   d\n",
            "Epoch 15. Time: 5.494, Train loss: 1.044\n",
            "h e l l o   i   g o t   t o   g e t   t h e   s t a r t   t h e   s t a r t   t h e   s t a r t   t h e   s t\n",
            "Epoch 16. Time: 5.479, Train loss: 1.039\n",
            "h e l l o   t h e   s e e n   t h e   s e e n   t h e   s e e n   t h e   s e e n   t h e   s e e n   t h e  \n",
            "Epoch 17. Time: 5.520, Train loss: 1.037\n",
            "h e l l o   i   c a n t   b e l i e v e   a   c o m e   t o   t h e   s e e m s   a n d   t h e   s e e m s  \n",
            "Epoch 18. Time: 5.470, Train loss: 1.035\n",
            "h e l l o   t h e   s t a r t   t h e   s t a r t   t h e   s t a r t   t h e   s t a r t   t h e   s t a r t\n",
            "Epoch 19. Time: 5.447, Train loss: 1.034\n",
            "h e l l o   s t a r t   a n d   i   c a n t   b e l i e v e   a   s t a r t   t h e   s t o p   t h e   s t o\n",
            "Epoch 20. Time: 5.532, Train loss: 1.032\n",
            "h e l l o   t h e   s c h o o l   t h e   s c h o o l   t h e   s c h o o l   t h e   s c h o o l   t h e   s\n",
            "Epoch 21. Time: 6.248, Train loss: 1.032\n",
            "h e l l o   t h e   s c r e a d   t h e   s c r e a d   t h e   s c r e a d   t h e   s c r e a d   t h e   s\n",
            "Epoch 22. Time: 5.460, Train loss: 1.033\n",
            "h e l l o   t h e   s t a r t s   a   l i t t l e   t h e   s t a r t s   a   l i t t l e   t h e   s t a r t\n",
            "Epoch 23. Time: 5.494, Train loss: 1.032\n",
            "h e l l o   s a y   t h e   s t a r t   t h e   s t a r t   t h e   s t a r t   t h e   s t a r t   t h e   s\n",
            "Epoch 24. Time: 5.543, Train loss: 1.030\n",
            "h e l l o   t h e   s t a r t   t h e   s t a r t   t h e   s t a r t   t h e   s t a r t   t h e   s t a r t\n",
            "Epoch 25. Time: 5.576, Train loss: 1.028\n",
            "h e l l o   s e e   t h e   s a v e   a   g o o d s   g o n n a   b e   t h e   s a v e   a   g o o d s   g o\n",
            "Epoch 26. Time: 5.567, Train loss: 1.029\n",
            "h e l l o   t h e   s t a r t   t h e   s t a r t   t h e   s t a r t   t h e   s t a r t   t h e   s t a r t\n",
            "Epoch 27. Time: 5.601, Train loss: 1.029\n",
            "h e l l o   i   c a n t   b e l i e v e   a   l i t t l e   t h i n k   t h e   s a y s   t h e   s a y s   t\n",
            "Epoch 28. Time: 5.551, Train loss: 1.030\n",
            "h e l l o   i   c a n   s e e   t h e   s t a r t e s t   t h e   s t a r t e s t   t h e   s t a r t e s t  \n",
            "Epoch 29. Time: 5.575, Train loss: 1.028\n",
            "h e l l o   t h e   s c h o o l   i s   a   l i t t l e   t h i n k   t h e   s c h o o l   i s   a   l i t t\n",
            "Epoch 30. Time: 5.508, Train loss: 1.028\n",
            "h e l l o   t h e   s t o p   t h e   s t o p   t h e   s t o p   t h e   s t o p   t h e   s t o p   t h e  \n",
            "Epoch 31. Time: 5.592, Train loss: 1.028\n",
            "h e l l o   i   t h i n k   i   t h i n k   i   t h i n k   i   t h i n k   i   t h i n k   i   t h i n k   i\n",
            "Epoch 32. Time: 5.535, Train loss: 1.028\n",
            "h e l l o   t h e   s e c o n d   t h e   s e e k   t h e   s e e k   t h e   s e e k   t h e   s e e k   t h\n",
            "Epoch 33. Time: 5.521, Train loss: 1.029\n",
            "h e l l o   i   c a n t   y o u   a r e   y o u   s e e   t h e   s t a r t\n",
            "Epoch 34. Time: 5.520, Train loss: 1.030\n",
            "h e l l o   i   c a n t   b e l i e v e   a   l i t t l e   t h e   s t o p   t h e   s t o p   t h e   s t o\n",
            "Epoch 35. Time: 5.551, Train loss: 1.029\n",
            "h e l l o   t h e   s t o r y   a n d   i   c a n t   b e l i e v e   a   l i t t l e   t h e   s t o r y   a\n",
            "Epoch 36. Time: 5.523, Train loss: 1.027\n",
            "h e l l o   i   c a n t   b e l i e v e   t h e   s t o p   t h e   s t o p   t h e   s t o p   t h e   s t o\n",
            "Epoch 37. Time: 5.529, Train loss: 1.026\n",
            "h e l l o   t h e   s t o p   t h e   s t o p   t h e   s t o p   t h e   s t o p   t h e   s t o p   t h e  \n",
            "Epoch 38. Time: 5.506, Train loss: 1.030\n",
            "h e l l o   i   c a n   s e e   a   l i t t l e   t h i s   s i m p s o n   a n d   i   c a n   s e e   a   l\n",
            "Epoch 39. Time: 5.508, Train loss: 1.029\n",
            "h e l l o   t h e   s t a t e   t h e   s t a t e   t h e   s t a t e   t h e   s t a t e   t h e   s t a t e\n",
            "Epoch 40. Time: 5.524, Train loss: 1.027\n",
            "h e l l o   t h e   s e c o n d   t h e   s e c o n d   t h e   s e c o n d   t h e   s e c o n d   t h e   s\n",
            "Epoch 41. Time: 5.506, Train loss: 1.025\n",
            "h e l l o   i   c a n t   b e l i e v e   a   l i t t l e   t h e   s t o r y   a n d   i   t h i n k   i   c\n",
            "Epoch 42. Time: 5.497, Train loss: 1.024\n",
            "h e l l o   s o m e t h i n g   t o   s e e   a   s i g n   t h e   s t u p i d   a   s i g n   t h e   s t u\n",
            "Epoch 43. Time: 5.488, Train loss: 1.026\n",
            "h e l l o   t h e   s t a r t s   a n d   i   d o n t   k n o w   i t s   a   l i t t l e   t h e   s t a r t\n",
            "Epoch 44. Time: 5.555, Train loss: 1.027\n",
            "h e l l o   t h e   s e e m s   a n d   i   c a n   t h e   s e e m s   a n d   i   c a n   t h e   s e e m s\n",
            "Epoch 45. Time: 5.501, Train loss: 1.027\n",
            "h e l l o   i   c a n t   b e l i e v e   a   s e e m s   a n d   i   c a n t   b e l i e v e   a   s e e m s\n",
            "Epoch 46. Time: 5.498, Train loss: 1.028\n",
            "h e l l o   t h e   s h o u l d   b e   t h e   s h o u l d   b e   t h e   s h o u l d   b e   t h e   s h o\n",
            "Epoch 47. Time: 5.485, Train loss: 1.025\n",
            "h e l l o   t h e   s t o r y   a n d   i   c a n t   b e l i e v e   a n d   i   c a n t   b e l i e v e   a\n",
            "Epoch 48. Time: 5.522, Train loss: 1.027\n",
            "h e l l o   i   c a n t   b e l i e v e   a   s t a y   a   s t a y   a   s t a y   a   s t a y   a   s t a y\n",
            "Epoch 49. Time: 5.510, Train loss: 1.026\n",
            "h e l l o   i   c a n t   y o u   a r e   s o   m o m   a r e   y o u   a r e   s o   m o m   a r e   y o u  \n",
            "Epoch 50. Time: 5.501, Train loss: 1.026\n",
            "h e l l o   i   c a n t   s e e   t h e   s t o r y   a n d   i   t h i n k   i   a m   t h e   s t o r y   a\n",
            "Epoch 51. Time: 5.464, Train loss: 1.025\n",
            "h e l l o   t h e   s t a y   a n d   i   t h i n k   i   t h i n k   i   t h i n k   i   t h i n k   i   t h\n",
            "Epoch 52. Time: 5.492, Train loss: 1.025\n",
            "h e l l o w e l   s o m e t h i n g   t h e   s t a r t   t h e   s t a r t   t h e   s t a r t   t h e   s t\n",
            "Epoch 53. Time: 5.479, Train loss: 1.026\n",
            "h e l l o   s e e   t h e   s t a r t   t h e   s t a r t   t h e   s t a r t   t h e   s t a r t   t h e   s\n",
            "Epoch 54. Time: 6.246, Train loss: 1.027\n",
            "h e l l o   i   t h i n k   i   t h i n k   i   t h i n k   i   t h i n k   i   t h i n k   i   t h i n k   i\n",
            "Epoch 55. Time: 5.493, Train loss: 1.030\n",
            "h e l l o   i   c a n t   b e l i e v e   a   s t o p\n",
            "Epoch 56. Time: 5.493, Train loss: 1.027\n",
            "h e l l o   i   t h i n k   i   t h i n k   i   t h i n k   i   t h i n k   i   t h i n k   i   t h i n k   i\n",
            "Epoch 57. Time: 5.483, Train loss: 1.025\n",
            "h e l l o   t h e y r e   a   s t o r y   a n d   i   c a n t   b e   a   s t o r y   a n d   i   c a n t   b\n",
            "Epoch 58. Time: 5.479, Train loss: 1.026\n",
            "h e l l o   i   c a n   m a k e   a   s t a n d   t h e   s t o r y   a n d   i   c a n   m a k e   a   s t a\n",
            "Epoch 59. Time: 5.469, Train loss: 1.024\n",
            "h e l l o   s e e   t h e   s t a r t e d   t h e   s t a r t e d   t h e   s t a r t e d   t h e   s t a r t\n",
            "Epoch 60. Time: 5.543, Train loss: 1.026\n",
            "h e l l o   t h e   s h o u l d   b e   a   l o t   a   l o t   a   l o t   a   l o t   a   l o t   a   l o t\n",
            "Epoch 61. Time: 5.460, Train loss: 1.024\n",
            "h e l l o   i   t h i n k   y o u   g o   t o   t h e   s t a r t e d   t h e   s t a r t e d   t h e   s t a\n",
            "Epoch 62. Time: 5.512, Train loss: 1.022\n",
            "h e l l o   s o m e t h i n g   t h e   s h e s   a n d   i   t h i n k   i   t h i n k   i   t h i n k   i  \n",
            "Epoch 63. Time: 5.514, Train loss: 1.025\n",
            "h e l l o   i   t h i n k   y o u   s e e   t h e   s t o p   t h e   s t o p   t h e   s t o p   t h e   s t\n",
            "Epoch 64. Time: 5.498, Train loss: 1.026\n",
            "h e l l o   t h e   s t u d y   a n d   i   t h i n k   i   t h i n k   i   t h i n k   i   t h i n k   i   t\n",
            "Epoch 65. Time: 5.481, Train loss: 1.025\n",
            "h e l l o   t h e   s t o r y   a n d   i   t h i n k   i   t h i n k   i   t h i n k   i   t h i n k   i   t\n",
            "Epoch 66. Time: 5.494, Train loss: 1.040\n",
            "h e l l o   t h e   s e e n   t h e   s e e n   t h e   s e e n   t h e   s e e n   t h e   s e e n   t h e  \n",
            "Epoch 67. Time: 5.503, Train loss: 1.029\n",
            "h e l l o   t h e   s t o p   t h e   s t o p   t h e   s t o p   t h e   s t o p   t h e   s t o p   t h e  \n",
            "Epoch 68. Time: 5.528, Train loss: 1.025\n",
            "h e l l o   t h e   s t a r t h   t h e   s t a r t h   t h e   s t a r t h   t h e   s t a r t h   t h e   s\n",
            "Epoch 69. Time: 5.516, Train loss: 1.028\n",
            "h e l l o   i   c a n t   y o u   s e e   t h e   s t u p i d   a n d   i   c a n t   y o u   s e e   t h e  \n",
            "Epoch 70. Time: 5.521, Train loss: 1.030\n",
            "h e l l o   t h e   s c h o o l\n",
            "Epoch 71. Time: 5.506, Train loss: 1.026\n",
            "h e l l o   t h e   s t a r t\n",
            "Epoch 72. Time: 5.482, Train loss: 1.024\n",
            "h e l l o   i   c a n t   b e l i e v e   a   l i t t l e   t o   m e   a n d   i   c a n t   b e l i e v e  \n",
            "Epoch 73. Time: 5.527, Train loss: 1.030\n",
            "h e l l o   i   c a n t   b e l i e v e   a   s t a n d   t h e   s t o p   t h e   s t o p   t h e   s t o p\n",
            "Epoch 74. Time: 5.537, Train loss: 1.028\n",
            "h e l l o   i   t h i n k   y o u   g o   t h e   s c h o o l   a n d   i   t h i n k   y o u   g o   t h e  \n",
            "Epoch 75. Time: 5.502, Train loss: 1.029\n",
            "h e l l o   i   c a n t   b e   a   l i t t l e   b e c a u s e   i   c a n t   b e   a   l i t t l e   b e c\n",
            "Epoch 76. Time: 5.515, Train loss: 1.025\n",
            "h e l l o   t h e   s e e m s   a n d   i t   a r e   y o u   d o n t   k n o w   i   c a n t   b e l i e v e\n",
            "Epoch 77. Time: 5.527, Train loss: 1.021\n",
            "h e l l o   i   c a n t   b e l i e v e   i t   t o   t h e   s e e s t   t h e   s e e m s   a   l o t   a  \n",
            "Epoch 78. Time: 5.511, Train loss: 1.035\n",
            "h e l l o   t h e   s t o r y   a n d   i   c a n   s t r a n s h a   t h i s   i s   a   l i t t l e   t h e\n",
            "Epoch 79. Time: 5.517, Train loss: 1.024\n",
            "h e l l o   s t o p   a n d   t h e   s t o p   a n d   t h e   s t o p   a n d   t h e   s t o p   a n d   t\n",
            "Epoch 80. Time: 5.537, Train loss: 1.025\n",
            "h e l l o   i t s   a   l i t t l e   t h e   s e e m s   a r e   y o u   g e t   t h e   s e e m s   a r e  \n",
            "Epoch 81. Time: 5.520, Train loss: 1.025\n",
            "h e l l o   a   l i t t l e   b e e n   t h e   s e e n   t h e   s e e n   t h e   s e e n   t h e   s e e n\n",
            "Epoch 82. Time: 5.525, Train loss: 1.023\n",
            "h e l l o   t h e   s e e m s   a n d   i   c a n t   y o u   a n d   i   c a n t   y o u   a n d   i   c a n\n",
            "Epoch 83. Time: 5.459, Train loss: 1.027\n",
            "h e l l o   t h e   s t a r t   a n d   i   c a n   m a k e   a   l i t t l e   t o   t h e   s t a r t   a n\n",
            "Epoch 84. Time: 5.462, Train loss: 1.026\n",
            "h e l l o   i   d o n t   k n o w   i   d o n t   k n o w   i   d o n t   k n o w   i   d o n t   k n o w   i\n",
            "Epoch 85. Time: 5.489, Train loss: 1.024\n",
            "h e l l o   i   c a n t   y o u   s e e   a   c o u l d   b e   a   l i t t l e   t h e   s c h o o l   a r e\n",
            "Epoch 86. Time: 5.469, Train loss: 1.020\n",
            "h e l l o   t h e   s t a r t   t h e   s t a r t   t h e   s t a r t   t h e   s t a r t   t h e   s t a r t\n",
            "Epoch 87. Time: 5.668, Train loss: 1.023\n",
            "h e l l o   t h e   s t a r t   t h e   s t a r t   t h e   s t a r t   t h e   s t a r t   t h e   s t a r t\n",
            "Epoch 88. Time: 6.025, Train loss: 1.019\n",
            "h e l l o   t h e   s c h o o l\n",
            "Epoch 89. Time: 5.504, Train loss: 1.020\n",
            "h e l l o   s h o u l d   s e e   t h e   s c h o o l   a   l i t t l e   t h e   s c h o o l   a   l i t t l\n",
            "Epoch 90. Time: 5.463, Train loss: 1.022\n",
            "h e l l o   i   c a n   s e e m s   a n d   t h e   s c h o o l\n",
            "Epoch 91. Time: 5.475, Train loss: 1.023\n",
            "h e l l o   t h e   s t a r t   t h e   s t a r t   t h e   s t a r t   t h e   s t a r t   t h e   s t a r t\n",
            "Epoch 92. Time: 5.481, Train loss: 1.024\n",
            "h e l l o   t h e   s c h o o l   a r e   t h e   s c h o o l   a r e   t h e   s c h o o l   a r e   t h e  \n",
            "Epoch 93. Time: 5.475, Train loss: 1.024\n",
            "h e l l o   t h i n k   i   c a n t   b e l i e v e   a n d   t h e   s t a r t e d   t h e   s t a r t e d  \n",
            "Epoch 94. Time: 5.446, Train loss: 1.026\n",
            "h e l l o   i   c a n t   b e l i e v e   a n d   i   c a n t   b e l i e v e   a n d   i   c a n t   b e l i\n",
            "Epoch 95. Time: 5.495, Train loss: 1.029\n",
            "h e l l o   i   c a n   y o u   s e e   y o u   s e e   y o u   s e e   y o u   s e e   y o u   s e e   y o u\n",
            "Epoch 96. Time: 5.495, Train loss: 1.030\n",
            "h e l l o   t h e   s c h o o l   a   s t a r t   t h e   s c h o o l   a   s t a r t   t h e   s c h o o l  \n",
            "Epoch 97. Time: 5.486, Train loss: 1.026\n",
            "h e l l o   t h e   s t a r t\n",
            "Epoch 98. Time: 5.474, Train loss: 1.022\n",
            "h e l l o   t h e   s c h o o l\n",
            "Epoch 99. Time: 5.482, Train loss: 1.030\n",
            "h e l l o   i   t h i n k   i   t h i n k   i   t h i n k   i   t h i n k   i   t h i n k   i   t h i n k   i\n",
            "Epoch 100. Time: 5.508, Train loss: 1.037\n",
            "h e l l o   t h e   s c h o o l   a n d   i   c a n   s e e   t h e   s c h o o l   a n d   i   c a n   s e e\n",
            "Epoch 101. Time: 5.523, Train loss: 1.025\n",
            "h e l l o   t h e   s t a r t   t h e   s t a r t   t h e   s t a r t   t h e   s t a r t   t h e   s t a r t\n",
            "Epoch 102. Time: 5.516, Train loss: 1.023\n",
            "h e l l o   i   c a n t   b e l i e v e   a   c o m e   t o   t h e   s c h o o l   a r e   y o u   d o n t  \n",
            "Epoch 103. Time: 5.523, Train loss: 1.024\n",
            "h e l l o   t h e   s c h o o l   a r e   y o u   a r e   y o u   a r e   y o u   a r e   y o u   a r e   y o\n",
            "Epoch 104. Time: 5.583, Train loss: 1.021\n",
            "h e l l o   t h e   s c h o o l   a r e   y o u   a r e   y o u   a r e   y o u   a r e   y o u   a r e   y o\n",
            "Epoch 105. Time: 5.565, Train loss: 1.022\n",
            "h e l l o   t h e   s c h o o l   a n d   t h e   s c h o o l   a n d   t h e   s c h o o l   a n d   t h e  \n",
            "Epoch 106. Time: 5.560, Train loss: 1.019\n",
            "h e l l o   t h e   s c h o o l\n",
            "Epoch 107. Time: 5.488, Train loss: 1.030\n",
            "h e l l o   i   c a n t   b e l i e v e   a n d   i   c a n t   b e l i e v e   a n d   i   c a n t   b e l i\n",
            "Epoch 108. Time: 5.498, Train loss: 1.049\n",
            "h e l l o   t h e   s e e m s   a r e   y o u   a r e   s h o u l d   b e   a   p r o b a b l y\n",
            "Epoch 109. Time: 5.497, Train loss: 1.034\n",
            "h e l l o   t h e   s t a r t\n",
            "Epoch 110. Time: 5.483, Train loss: 1.028\n",
            "h e l l o   t h e   s c h o o l   t h e   s c h o o l   t h e   s c h o o l   t h e   s c h o o l   t h e   s\n",
            "Epoch 111. Time: 5.505, Train loss: 1.027\n",
            "h e l l o   t h e r e s   a   l i t t l e   t h e   s c h o o l\n",
            "Epoch 112. Time: 5.550, Train loss: 1.031\n",
            "h e l l o   t h e   s c h o o l\n",
            "Epoch 113. Time: 5.568, Train loss: 1.022\n",
            "h e l l o   t h e   s c h o o l\n",
            "Epoch 114. Time: 5.546, Train loss: 1.030\n",
            "h e l l o   t h e   s e e m s   a n d   i   t h i n k   i   t h i n k   i   t h i n k   i   t h i n k   i   t\n",
            "Epoch 115. Time: 5.514, Train loss: 1.026\n",
            "h e l l o   t h e   s c h o o l   a n d   i   c a n   s t a r t e d   t o   t h e   s c h o o l   a n d   i  \n",
            "Epoch 116. Time: 5.493, Train loss: 1.025\n",
            "h e l l o   t h e   s c h o o l\n",
            "Epoch 117. Time: 5.527, Train loss: 1.024\n",
            "h e l l o   t h e   s t o r y   a n d   t h e   s t o r y   a n d   t h e   s t o r y   a n d   t h e   s t o\n",
            "Epoch 118. Time: 5.506, Train loss: 1.028\n",
            "h e l l o   t h e   s e e m   a   s t o r y   a   s t o r y   a   s t o r y   a   s t o r y   a   s t o r y  \n",
            "Epoch 119. Time: 5.529, Train loss: 1.024\n",
            "h e l l o   t h e   s c h o o l   a n d   i   c a n t   b e l i e v e   a   l i t t l e   t h e   s c h o o l\n",
            "Epoch 120. Time: 5.540, Train loss: 1.023\n",
            "h e l l o   t h e   s c h o o l   a n d   i   c a n t   b e l i e v e   a n d   i   c a n t   b e l i e v e  \n",
            "Epoch 121. Time: 6.319, Train loss: 1.022\n",
            "h e l l o   t h e   s e e m s   a n d   i   c a n t   b e l i e v e   a n d   i   c a n t   b e l i e v e   a\n",
            "Epoch 122. Time: 5.551, Train loss: 1.023\n",
            "h e l l o   t h e   s c h o o l   a r e   y o u   g e t   t h e   s c h o o l   a r e   y o u   g e t   t h e\n",
            "Epoch 123. Time: 5.521, Train loss: 1.023\n",
            "h e l l o   t h e   s e e m s   a n d   t h e   s e e m s   a n d   t h e   s e e m s   a n d   t h e   s e e\n",
            "Epoch 124. Time: 5.519, Train loss: 1.045\n",
            "h e l l o   t h e   s e e m s   a   l i t t l e   t h e   s e e m s   a   l i t t l e   t h e   s e e m s   a\n",
            "Epoch 125. Time: 5.459, Train loss: 1.037\n",
            "h e l l o   t h e   s c h o o l   a n d   i   c a n t   s e e   t h e   s c h o o l   a n d   i   c a n t   s\n",
            "Epoch 126. Time: 5.506, Train loss: 1.029\n",
            "h e l l o   t h e   s c h o o l   a r e   y o u   g o t   t o   s e e   t h e   s c h o o l   a r e   y o u  \n",
            "Epoch 127. Time: 5.520, Train loss: 1.027\n",
            "h e l l o   t h e   s c h o o l   i   t h i n k   i   t h i n k   i   t h i n k   i   t h i n k   i   t h i n\n",
            "Epoch 128. Time: 5.480, Train loss: 1.024\n",
            "h e l l o   t h e   s c h o o l   a r e   y o u   g o t   t o   s e e   i t s   t h e   s c h o o l   a r e  \n",
            "Epoch 129. Time: 5.519, Train loss: 1.024\n",
            "h e l l o   t h e   s c h o o l\n",
            "Epoch 130. Time: 5.498, Train loss: 1.020\n",
            "h e l l o   t h e   s c h o o l   a n d   t h e   s c h o o l   a n d   t h e   s c h o o l   a n d   t h e  \n",
            "Epoch 131. Time: 5.492, Train loss: 1.034\n",
            "h e l l o   t h e   c a r e   t o   t h e   c a r e   t o   t h e   c a r e   t o   t h e   c a r e   t o   t\n",
            "Epoch 132. Time: 5.480, Train loss: 1.044\n",
            "h e l l o   t h e   s c h o o l\n",
            "Epoch 133. Time: 5.513, Train loss: 1.029\n",
            "h e l l o   t h e   s c h o o l   a   l i t t l e   t h e   s c h o o l   a   l i t t l e   t h e   s c h o o\n",
            "Epoch 134. Time: 5.484, Train loss: 1.028\n",
            "h e l l o   t h e   s t o r y   a n d   i   c a n t   b e l i e v e   t h e   s t o r y   a n d   i   c a n t\n",
            "Epoch 135. Time: 5.531, Train loss: 1.027\n",
            "h e l l o   t h e   s c h o o l   a n d   t h e   s c h o o l   a n d   t h e   s c h o o l   a n d   t h e  \n",
            "Epoch 136. Time: 5.545, Train loss: 1.029\n",
            "h e l l o   t h e   s e n t e r   a n d   t h e   s e n t e r   a n d   t h e   s e n t e r   a n d   t h e  \n",
            "Epoch 137. Time: 5.511, Train loss: 1.027\n",
            "h e l l o   t h e   s c h o o l   a n d   t h e   s c h o o l   a n d   t h e   s c h o o l   a n d   t h e  \n",
            "Epoch 138. Time: 5.513, Train loss: 1.024\n",
            "h e l l o   t h e   s c h o o l\n",
            "Epoch 139. Time: 5.500, Train loss: 1.028\n",
            "h e l l o   t h e   s e e n   t h e   s e e n   t h e   s e e n   t h e   s e e n   t h e   s e e n   t h e  \n",
            "Epoch 140. Time: 5.484, Train loss: 1.025\n",
            "h e l l o   t h e   s e e   t h e   s e e   t h e   s e e   t h e   s e e   t h e   s e e   t h e   s e e   t\n",
            "Epoch 141. Time: 5.472, Train loss: 1.029\n",
            "h e l l o   t h e   s c h o o l\n",
            "Epoch 142. Time: 5.443, Train loss: 1.026\n",
            "h e l l o   t h e   c o u n t i n g   t h e   s c h o o l   a r e   y o u   s e e   t h e   s c h o o l   a r\n",
            "Epoch 143. Time: 5.519, Train loss: 1.023\n",
            "h e l l o   t h e r e s   a   l i t t l e   b e e n   t h e   s c h o o l   a n d   i   c a n t   b e l i e v\n",
            "Epoch 144. Time: 5.502, Train loss: 1.022\n",
            "h e l l o   t h e   s c h o o l   a n d   t h e   s c h o o l   a n d   t h e   s c h o o l   a n d   t h e  \n",
            "Epoch 145. Time: 5.513, Train loss: 1.020\n",
            "h e l l o   t h e r e   a   l i t t l e   t h e   s c h o o l   a r e   y o u   g o   t o   t h e   s c h o o\n",
            "Epoch 146. Time: 5.483, Train loss: 1.021\n",
            "h e l l o   t h e   s c h o o l   a n d   t h e   s c h o o l   a n d   t h e   s c h o o l   a n d   t h e  \n",
            "Epoch 147. Time: 5.509, Train loss: 1.020\n",
            "h e l l o   t h e r e s   a   s t o p   t h e   s c h o o l   a n d   t h e   s c h o o l   a n d   t h e   s\n",
            "Epoch 148. Time: 5.471, Train loss: 1.019\n",
            "h e l l o   t h e   s c h o o l   a n d   t h e   s c h o o l   a n d   t h e   s c h o o l   a n d   t h e  \n",
            "Epoch 149. Time: 5.495, Train loss: 1.024\n",
            "h e l l o   t h e   s c h o o l   a r e   y o u   g o   t h e   s c h o o l   a r e   y o u   g o   t h e   s\n",
            "Epoch 150. Time: 5.523, Train loss: 1.021\n",
            "h e l l o   t h e   s c h o o l   a n d   i   c a n   s e e   t h e   s c h o o l   a n d   i   c a n   s e e\n",
            "Epoch 151. Time: 5.490, Train loss: 1.025\n",
            "h e l l o   t h e   s c h o o l   a n d   t h e   s c h o o l   a n d   t h e   s c h o o l   a n d   t h e  \n",
            "Epoch 152. Time: 5.467, Train loss: 1.022\n",
            "h e l l o   t h e   s c h o o l   a n d   i   c a n t   b e l i e v e   a n d   i   c a n t   b e l i e v e  \n",
            "Epoch 153. Time: 5.683, Train loss: 1.022\n",
            "h e l l o   t h e r e s   a   l i t t l e   t h e   s c h o o l   a n d   t h e   s c h o o l   a n d   t h e\n",
            "Epoch 154. Time: 6.079, Train loss: 1.020\n",
            "h e l l o   t h e r e s   a   l i t t l e   b e e n   t h e   s c h o o l   a n d   t h e   s c h o o l   a n\n",
            "Epoch 155. Time: 5.510, Train loss: 1.017\n",
            "h e l l o   t h e   s c h o o l   a n d   t h e   s c h o o l   a n d   t h e   s c h o o l   a n d   t h e  \n",
            "Epoch 156. Time: 5.492, Train loss: 1.018\n",
            "h e l l o   t h e   s c h o o l\n",
            "Epoch 157. Time: 5.491, Train loss: 1.023\n",
            "h e l l o   i   c a n t   b e l i e v e   a n d   t h e   m o n e y   t h e   s c h o o l   a n d   t h e   m\n",
            "Epoch 158. Time: 5.509, Train loss: 1.021\n",
            "h e l l o   t h e   s c h o o l   a n d   t h e   s c h o o l   a n d   t h e   s c h o o l   a n d   t h e  \n",
            "Epoch 159. Time: 5.548, Train loss: 1.040\n",
            "h e l l o   t h e   s c h o o l   a   s i m p s o n   a   s i g n   a n d   t h e   s c h o o l   a   s i m p\n",
            "Epoch 160. Time: 5.563, Train loss: 1.024\n",
            "h e l l o   t h e   s c h o o l   a n d   t h e   s c h o o l   a n d   t h e   s c h o o l   a n d   t h e  \n",
            "Epoch 161. Time: 5.505, Train loss: 1.022\n",
            "h e l l o   t h e   s t o p   t h e   s t o p   t h e   s t o p   t h e   s t o p   t h e   s t o p   t h e  \n",
            "Epoch 162. Time: 5.507, Train loss: 1.023\n",
            "h e l l o   t h e r e s   a   s t a r t   t h e   s t a r t   a n d   t h e   s t a r t   a n d   t h e   s t\n",
            "Epoch 163. Time: 5.522, Train loss: 1.019\n",
            "h e l l o   t h e   s c h o o l   a n d   t h e   s c h o o l   a n d   t h e   s c h o o l   a n d   t h e  \n",
            "Epoch 164. Time: 5.532, Train loss: 1.022\n",
            "h e l l o   t h e   s c h o o l   a n d   t h e   s c h o o l   a n d   t h e   s c h o o l   a n d   t h e  \n",
            "Epoch 165. Time: 5.552, Train loss: 1.035\n",
            "h e l l o   t h e r e s   a   s t a r t s   a n d   i   c a n   s e e   t h e   f r o m   t h e   f r o m   t\n",
            "Epoch 166. Time: 5.522, Train loss: 1.025\n",
            "h e l l o   t h e   s c h o o l   a n d   i   c a n t   y o u   t h i n k   i   c a n t   y o u   t h i n k  \n",
            "Epoch 167. Time: 5.536, Train loss: 1.116\n",
            "h e l l o   t h e r e s   t h e   s e e   t h e   s e e   t h e   s e e   t h e   s e e   t h e   s e e   t h\n",
            "Epoch 168. Time: 5.579, Train loss: 1.075\n",
            "h e l l o   t h e   s c h o o l   a n d   t h e   s c h o o l   a n d   t h e   s c h o o l   a n d   t h e  \n",
            "Epoch 169. Time: 5.527, Train loss: 1.055\n",
            "h e l l o   t h e   s c h o o l   a n d   i   c a n t   g o   t h e   s c h o o l   a n d   i   c a n t   g o\n",
            "Epoch 170. Time: 5.510, Train loss: 1.054\n",
            "h e l l o   t h e r e s   a   s c h o o l   t h e   s c h o o l   t h e   s c h o o l   t h e   s c h o o l  \n",
            "Epoch 171. Time: 5.526, Train loss: 1.044\n",
            "h e l l o   t h e   s c h o o l   a n d   t h e   s c h o o l   a n d   t h e   s c h o o l   a n d   t h e  \n",
            "Epoch 172. Time: 5.470, Train loss: 1.039\n",
            "h e l l o   t h e   s c h o o l\n",
            "Epoch 173. Time: 5.490, Train loss: 1.037\n",
            "h e l l o   t h e r e s   a   s t o p   t h e   s c h o o l   a n d   t h e r e s   a   s t o p   t h e   s c\n",
            "Epoch 174. Time: 5.466, Train loss: 1.033\n",
            "h e l l o   t h e   s c r e a l   a n d   t h e   s c r e a l   a n d   t h e   s c r e a l   a n d   t h e  \n",
            "Epoch 175. Time: 5.516, Train loss: 1.030\n",
            "h e l l o   t h e   s c h o o l   a n d   i   c a n t   g o   t h e   s c h o o l   a n d   i   c a n t   g o\n",
            "Epoch 176. Time: 5.481, Train loss: 1.030\n",
            "h e l l o   a n d   t h e   s c h o o l   a n d   t h e   s c h o o l   a n d   t h e   s c h o o l   a n d  \n",
            "Epoch 177. Time: 5.489, Train loss: 1.029\n",
            "h e l l o   t h e   s c h o o l   a n d   i   c a n   s e e   t h e   s c h o o l   a n d   i   c a n   s e e\n",
            "Epoch 178. Time: 5.504, Train loss: 1.029\n",
            "h e l l o w   t h e   s c h o o l\n",
            "Epoch 179. Time: 5.521, Train loss: 1.026\n",
            "h e l l o   t h e   s c h o o l\n",
            "Epoch 180. Time: 5.502, Train loss: 1.029\n",
            "h e l l o   t h e r e s   a   s t o p   t h e   s c h o o l   a n d   i   c a n t   b e l i e v e   a   s t o\n",
            "Epoch 181. Time: 5.454, Train loss: 1.024\n",
            "h e l l o   t h e   s c h o o l   a n d   t h e   s c h o o l   a n d   t h e   s c h o o l   a n d   t h e  \n",
            "Epoch 182. Time: 5.524, Train loss: 1.024\n",
            "h e l l o   s t a r t s   a n d   t h e   s c h o o l   a n d   t h e   s c h o o l   a n d   t h e   s c h o\n",
            "Epoch 183. Time: 5.519, Train loss: 1.025\n",
            "h e l l o   t h e r e s   t h e   s c h o o l   a n d   t h e   s c h o o l   a n d   t h e   s c h o o l   a\n",
            "Epoch 184. Time: 5.503, Train loss: 1.023\n",
            "h e l l o   t h e y   a l l   t h e   s c h o o l   a n d   i   c a n t   s e e   t h e   s c h o o l   a n d\n",
            "Epoch 185. Time: 5.498, Train loss: 1.024\n",
            "h e l l o   t h e y   s e e   t h e   s c h o o l   a n d   t h e   s c h o o l   a n d   t h e   s c h o o l\n",
            "Epoch 186. Time: 6.282, Train loss: 1.019\n",
            "h e l l o   s a y s   a n d   t h e   s c h o o l   a n d   t h e   s c h o o l   a n d   t h e   s c h o o l\n",
            "Epoch 187. Time: 5.529, Train loss: 1.024\n",
            "h e l l o   i   c a n t   b e l i e v e   t h e   s c h o o l   a n d   i   c a n t   b e l i e v e   t h e  \n",
            "Epoch 188. Time: 5.504, Train loss: 1.028\n",
            "h e l l o   t h e   s e r s t   a n d   i   c a n t   b e   t h e   s e r s t   a n d   i   c a n t   b e   t\n",
            "Epoch 189. Time: 5.515, Train loss: 1.026\n",
            "h e l l o   t h e r e s   t h e   s c h o o l\n",
            "Epoch 190. Time: 5.503, Train loss: 1.020\n",
            "h e l l o   t h e   s t a r t   t h e   s t a r t   t h e   s t a r t   t h e   s t a r t   t h e   s t a r t\n",
            "Epoch 191. Time: 5.526, Train loss: 1.018\n",
            "h e l l o   t h e r e s   a   l i t t l e   t o   t h e   s c h o o l   a n d   t h e   s c h o o l   a n d  \n",
            "Epoch 192. Time: 5.494, Train loss: 1.017\n",
            "h e l l o   t h e   s c h o o l   a n d   i   c a n t   b e l i e v e   a   l i t t l e   t o   s e e   t h e\n",
            "Epoch 193. Time: 5.500, Train loss: 1.019\n",
            "h e l l o   t h e   s c h o o l   a n d   i   c a n   m e a n   t h e   s c h o o l   a n d   i   c a n   m e\n",
            "Epoch 194. Time: 5.540, Train loss: 1.021\n",
            "h e l l o   t h e   s c h o o l   a   l i t t l e   t h e   s c h o o l   a   l i t t l e   t h e   s c h o o\n",
            "Epoch 195. Time: 5.514, Train loss: 1.018\n",
            "h e l l o   t h e   s c h o o l   a n d   t h e   s c h o o l   a n d   t h e   s c h o o l   a n d   t h e  \n",
            "Epoch 196. Time: 5.470, Train loss: 1.019\n",
            "h e l l o   t h e   s e e m s   a n d   t h e   s e e m s   a n d   t h e   s e e m s   a n d   t h e   s e e\n",
            "Epoch 197. Time: 5.522, Train loss: 1.018\n",
            "h e l l o   i   c a n   s e e   t h e   s c h o o l   a n d   i   c a n   s e e   t h e   s c h o o l   a n d\n",
            "Epoch 198. Time: 5.513, Train loss: 1.016\n",
            "h e l l o   t h e   s c h o o l   a n d   t h e   s c h o o l   a n d   t h e   s c h o o l   a n d   t h e  \n",
            "Epoch 199. Time: 5.517, Train loss: 1.016\n",
            "h e l l o   s h e s   t h e   s c h o o l   a r e   y o u   g o n n a   s e e   t h e   s c h o o l   a r e  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Как видим, обучение происходит достаточно медленно и затейливо... Тем не менее, замена оптимизатора с SGD на AdamW и уменьшение LR с 0.05 до 0.01 привели к улучшению процесса обучения RNN."
      ],
      "metadata": {
        "id": "gWtKPHjxDV0S"
      }
    }
  ]
}