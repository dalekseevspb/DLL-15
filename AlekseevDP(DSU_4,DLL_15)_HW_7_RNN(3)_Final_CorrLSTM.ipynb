{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## AlekseevDP(DSU-4,DLL-15)_HW#7_Рекуррентные НС (Часть 3). Исправлен LSTM\n",
        "\n",
        "Задание:\n",
        "- Возьмите англо-русскую пару фраз (https://www.manythings.org/anki/)\n",
        "- Обучите на них seq2seq по аналогии с занятием. Оцените полученное качество\n",
        "- Попробуйте добавить +1 рекуррентный слой в encoder и decoder\n",
        "- Попробуйте заменить GRU-ячейки на LSTM-ячейки\n",
        "\n",
        "Оцените качество во всех случаях\n",
        "\n"
      ],
      "metadata": {
        "id": "CtBL4V8vfsWC"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-YlRH3mQM9tf"
      },
      "source": [
        "%matplotlib inline"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MIEGXF8oM9tt"
      },
      "source": [
        "from io import open\n",
        "import unicodedata\n",
        "import string\n",
        "import re\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "CUDA_LAUNCH_BLOCKING = \"1\""
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !wget https://www.manythings.org/anki/rus-eng.zip\n",
        "# !unzip rus-eng.zip"
      ],
      "metadata": {
        "id": "6jChhs7oEzGg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Файл https://www.manythings.org/anki/rus-eng.zip потребовал предобработки в программе Anki, а именно - удаления колонки со служебной информацией (номер  карточки перевода, автор и т.п.), иначе при загрузке корпуса возникала ошибка.Обработанный файл: eng-rus_prepared_full.txt (для быстрого примера см. в репозитории Github файл 'eng-rus_prepared_example.txt')"
      ],
      "metadata": {
        "id": "KXhRJh3Ohr8q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# переместим 'eng-rus_prepared_full.txt' в каталог '/data' и переименуем в 'eng-rus.txt'\n",
        "!mkdir '/content/data'\n",
        "!mv eng-rus_prepared_full.txt '/content/data/eng-rus.txt'"
      ],
      "metadata": {
        "id": "Ty4GvpoccaLh"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "twIcAJnyRkW-",
        "outputId": "d1bc0358-ec25-4912-b212-5f2d01578209",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!head -50 data/eng-rus.txt"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Go.\tМарш!\n",
            "Hi.\tЗдравствуйте.\n",
            "Run!\tБеги!\n",
            "Run.\tБеги!\n",
            "Who?\tКто?\n",
            "Wow!\tВот это да!\n",
            "Duck!\tПригнись!\n",
            "Fire!\tОгонь!\n",
            "Help!\tПомогите!\n",
            "Hide.\tПрячься.\n",
            "Jump!\tПрыгай!\n",
            "Jump.\tПрыгай!\n",
            "Stay.\tОставайся.\n",
            "Stop!\tСтой!\n",
            "Wait!\tПодожди!\n",
            "Wait.\tЖдите.\n",
            "Do it.\tСделай это.\n",
            "Go on.\tПродолжай.\n",
            "Hello!\tЗдравствуйте.\n",
            "Hurry!\tПоспешите.\n",
            "I ran.\tЯ бежал.\n",
            "I see.\tПонимаю.\n",
            "I try.\tЯ пытаюсь.\n",
            "I won!\tЯ победил!\n",
            "Oh no!\tО нет!\n",
            "Relax.\tРасслабьтесь.\n",
            "Shoot!\tСтреляй!\n",
            "Smile.\tУлыбочка.\n",
            "Sorry?\tИзвините?\n",
            "Attack!\tВ атаку!\n",
            "Buy it.\tКупите её.\n",
            "Cheers!\tЗа ваше здоровье!\n",
            "Eat it.\tСъешь это.\n",
            "Eat up.\tДоедай.\n",
            "Freeze!\tНи с места!\n",
            "Get up.\tВставай.\n",
            "Go now.\tА теперь уходи.\n",
            "Got it!\tПонял!\n",
            "Got it?\tПонял?\n",
            "He ran.\tОн бежал.\n",
            "Hop in.\tЗалезай.\n",
            "Hug me.\tОбними меня.\n",
            "I fell.\tЯ упал.\n",
            "I knit.\tЯ вяжу.\n",
            "I know.\tЯ знаю.\n",
            "I left.\tЯ ушёл.\n",
            "I lied.\tЯ солгал.\n",
            "I lost.\tЯ проиграл.\n",
            "I paid.\tЯ заплатил.\n",
            "I pass.\tЯ пас.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kyNnJyruM9t1"
      },
      "source": [
        "SOS_token = 0\n",
        "EOS_token = 1\n",
        "\n",
        "\n",
        "class Lang:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.word2index = {}\n",
        "        self.word2count = {}\n",
        "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
        "        self.n_words = 2  # Count SOS and EOS\n",
        "\n",
        "    def addSentence(self, sentence):\n",
        "        for word in sentence.split(' '):\n",
        "            self.addWord(word)\n",
        "\n",
        "    def addWord(self, word):\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = self.n_words\n",
        "            self.word2count[word] = 1\n",
        "            self.index2word[self.n_words] = word\n",
        "            self.n_words += 1\n",
        "        else:\n",
        "            self.word2count[word] += 1"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Через библиотеку Unicodedata не удалось \"забороть\" кодировку предложений на русском языке, хотя кодировка UTF-8 включает в себя в т.ч. русские буквы (ошибка не возникала, но русскоязычная часть корпуса получалась всегда пустой - 0 фраз). Поэтому нашел альтернативный вариант - русские фразы переводить в транслит с использованием другой библиотеки (Unidecode):"
      ],
      "metadata": {
        "id": "-Zw-0lHtV7ga"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install Unidecode"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y89hifz8H1sH",
        "outputId": "23a3a596-ebdc-478d-9cfb-ba52d5444fad"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting Unidecode\n",
            "  Downloading Unidecode-1.3.6-py3-none-any.whl (235 kB)\n",
            "\u001b[K     |████████████████████████████████| 235 kB 7.7 MB/s \n",
            "\u001b[?25hInstalling collected packages: Unidecode\n",
            "Successfully installed Unidecode-1.3.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from unidecode import unidecode"
      ],
      "metadata": {
        "id": "6Z3NHaB8HsFs"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FXKs8j4bM9t6"
      },
      "source": [
        "def unicodeToAscii(s):\n",
        "    return ''.join(\n",
        "        # c for c in unicodedata.normalize('NFD', s)  # Turn a Unicode string to plain ASCII, \n",
        "        # if unicodedata.category(c) != 'Mn'          # thanks to http://stackoverflow.com/a/518232/2809427\n",
        "        c for c in unidecode(s)\n",
        ")\n",
        "\n",
        "# Lowercase, trim, and remove non-letter characters\n",
        "def normalizeString(s):\n",
        "    s = unicodeToAscii(s.lower().strip())\n",
        "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
        "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
        "    return s"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D8T4VxZeM9t-"
      },
      "source": [
        "def readLangs(lang1, lang2, reverse=False):\n",
        "    print(\"Reading lines...\")\n",
        "\n",
        "    # Read the file and split into lines\n",
        "    lines = open('data/%s-%s.txt' % (lang1, lang2), encoding='utf-8').\\\n",
        "        read().strip().split('\\n')\n",
        "\n",
        "    # Split every line into pairs and normalize\n",
        "    pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
        "\n",
        "    # Reverse pairs, make Lang instances\n",
        "    if reverse:\n",
        "        pairs = [list(reversed(p)) for p in pairs]\n",
        "        input_lang = Lang(lang2)\n",
        "        output_lang = Lang(lang1)\n",
        "    else:\n",
        "        input_lang = Lang(lang1)\n",
        "        output_lang = Lang(lang2)\n",
        "\n",
        "    return input_lang, output_lang, pairs"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## На \"типовых\" префиксах начала предложений (из туториала/лекции) модель Seq2seq на 1-слойных GRU-энкодере/декодере показывала хорошую точность предсказаний (лосс ниже 1.0), т.к. корпус был небольшой и предложения короткие. Поэтому с целью эксперимента задача была усложнена: увеличена длина предложения до 100 символов, а также добавлены новые префиксы начала предложений. Итоговый корпус для обучения модели составил 29743 фразы (см.ниже)."
      ],
      "metadata": {
        "id": "y5XjTYr3jI2i"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eBOwgEBdM9uB"
      },
      "source": [
        "MAX_LENGTH = 100\n",
        "\n",
        "eng_prefixes = (\n",
        "    \"i am\", \"i m\",\n",
        "    \"he is\", \"he s\",\n",
        "    \"she is\", \"she s\",\n",
        "    \"you are\", \"you re\",\n",
        "    \"we are\", \"we re\",\n",
        "    \"they are\", \"they re\",\n",
        "    \"i can\", \"i can t\",\n",
        "    \"that is\", \"that s\",\n",
        "    \"we will\", \"we ll\",\n",
        "    \"i was\", \"he was\", \"she was\",\n",
        "    \"do you\", \"are you\",\n",
        "    \"i have\", \"i ve\", \"i had\", \"i d\",\n",
        "    \"what is\", \"what s\", \"what was\",\n",
        "    \"what do\", \"what did\",\n",
        "    \"we have\", \"we ve\",\n",
        "    \"i do\", \"i don t\",\n",
        "    \"did\", \"i ll\",\n",
        "    \"there is\", \"there s\",\n",
        "    \"how\", \"when\"\n",
        ")\n",
        "\n",
        "def filterPair(p):\n",
        "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
        "        len(p[1].split(' ')) < MAX_LENGTH and \\\n",
        "        p[1].startswith(eng_prefixes)\n",
        "\n",
        "def filterPairs(pairs):\n",
        "    return [pair for pair in pairs if filterPair(pair)]"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6dZOGjd5M9uE",
        "outputId": "866369b3-cb1a-4627-ea5a-44a244781d93",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def prepareData(lang1, lang2, reverse=False):\n",
        "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n",
        "    print(\"Read %s sentence pairs\" % len(pairs))\n",
        "    pairs = filterPairs(pairs)\n",
        "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
        "    print(\"Counting words...\")\n",
        "    for pair in pairs:\n",
        "        input_lang.addSentence(pair[0])\n",
        "        output_lang.addSentence(pair[1])\n",
        "    print(\"Counted words:\")\n",
        "    print(input_lang.name, input_lang.n_words)\n",
        "    print(output_lang.name, output_lang.n_words)\n",
        "    return input_lang, output_lang, pairs\n",
        "\n",
        "input_lang, output_lang, pairs = prepareData('eng', 'rus', True)\n",
        "print(random.choice(pairs))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading lines...\n",
            "Read 746410 sentence pairs\n",
            "Trimmed to 29743 sentence pairs\n",
            "Counting words...\n",
            "Counted words:\n",
            "rus 13446\n",
            "eng 6084\n",
            "['vy delaete eto chashche raza v nedeliu ?', 'do you do that more than once a week ?']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(random.choice(pairs))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P9jQLXMv4VSY",
        "outputId": "3aed56ce-bf97-49f7-942c-6c7b9d0ca415"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['ia vizhu vershinu gory .', 'i can see the top of the mountain .']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vgtWqznCM9uH"
      },
      "source": [
        "The Encoder (1-layer GRU)\n",
        "-----------\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m9vm9QBWM9uI"
      },
      "source": [
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        output = embedded\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FwLTlgSyM9uK"
      },
      "source": [
        "The Decoder (1-layer GRU)\n",
        "-----------\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PFbuUL1LM9uL"
      },
      "source": [
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size):\n",
        "        super(DecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        output = self.embedding(input).view(1, 1, -1)\n",
        "        output = F.relu(output)\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        output = self.softmax(self.out(output[0]))\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Определим функции преобразования данных и обучения модели."
      ],
      "metadata": {
        "id": "pz-f4DgnZx3f"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z6gGPtXFM9uQ"
      },
      "source": [
        "def indexesFromSentence(lang, sentence):\n",
        "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
        "\n",
        "\n",
        "def tensorFromSentence(lang, sentence):\n",
        "    indexes = indexesFromSentence(lang, sentence)\n",
        "    indexes.append(EOS_token)\n",
        "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
        "\n",
        "\n",
        "def tensorsFromPair(pair):\n",
        "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
        "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
        "    return (input_tensor, target_tensor)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Fn8VDv8M9uS"
      },
      "source": [
        "teacher_forcing_ratio = 0.5\n",
        "\n",
        "\n",
        "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
        "    encoder_hidden = encoder.initHidden()\n",
        "\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "\n",
        "    input_length = input_tensor.size(0)\n",
        "    target_length = target_tensor.size(0)\n",
        "\n",
        "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "    loss = 0\n",
        "\n",
        "    for ei in range(input_length):\n",
        "        encoder_output, encoder_hidden = encoder(\n",
        "            input_tensor[ei], encoder_hidden)\n",
        "        encoder_outputs[ei] = encoder_output[0, 0]\n",
        "\n",
        "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
        "\n",
        "    decoder_hidden = encoder_hidden\n",
        "\n",
        "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
        "\n",
        "    if use_teacher_forcing:\n",
        "        # Teacher forcing: Feed the target as the next input\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden = decoder(\n",
        "                decoder_input, decoder_hidden)\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            decoder_input = target_tensor[di]  # Teacher forcing\n",
        "\n",
        "    else:\n",
        "        # Without teacher forcing: use its own predictions as the next input\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden = decoder(\n",
        "                decoder_input, decoder_hidden)\n",
        "            topv, topi = decoder_output.topk(1)\n",
        "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
        "\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            if decoder_input.item() == EOS_token:\n",
        "                break\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    return loss.item() / target_length"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EKsdwPmSM9uU"
      },
      "source": [
        "import time\n",
        "import math\n",
        "\n",
        "def asMinutes(s):\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)\n",
        "\n",
        "def timeSince(since, percent):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    es = s / (percent)\n",
        "    rs = es - s\n",
        "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C_z_k5IiM9uX"
      },
      "source": [
        "def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):\n",
        "    start = time.time()\n",
        "    plot_losses = []\n",
        "    print_loss_total = 0  # Reset every print_every\n",
        "    plot_loss_total = 0  # Reset every plot_every\n",
        "\n",
        "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
        "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
        "    training_pairs = [tensorsFromPair(random.choice(pairs))\n",
        "                      for i in range(n_iters)]\n",
        "    criterion = nn.NLLLoss()\n",
        "\n",
        "    for iter in range(1, n_iters + 1):\n",
        "        training_pair = training_pairs[iter - 1]\n",
        "        input_tensor = training_pair[0]\n",
        "        target_tensor = training_pair[1]\n",
        "\n",
        "        loss = train(input_tensor, target_tensor, encoder,\n",
        "                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
        "        print_loss_total += loss\n",
        "        plot_loss_total += loss\n",
        "\n",
        "        if iter % print_every == 0:\n",
        "            print_loss_avg = print_loss_total / print_every\n",
        "            print_loss_total = 0\n",
        "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
        "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
        "\n",
        "        if iter % plot_every == 0:\n",
        "            plot_loss_avg = plot_loss_total / plot_every\n",
        "            plot_losses.append(plot_loss_avg)\n",
        "            plot_loss_total = 0\n",
        "\n",
        "    showPlot(plot_losses)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0JXG-RzCM9uZ"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.switch_backend('agg')\n",
        "import matplotlib.ticker as ticker\n",
        "import numpy as np\n",
        "\n",
        "def showPlot(points):\n",
        "    plt.figure()\n",
        "    fig, ax = plt.subplots()\n",
        "    # this locator puts ticks at regular intervals\n",
        "    loc = ticker.MultipleLocator(base=0.2)\n",
        "    ax.yaxis.set_major_locator(loc)\n",
        "    plt.plot(points)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Bxf45h6M9ud"
      },
      "source": [
        "def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
        "    with torch.no_grad():\n",
        "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
        "        input_length = input_tensor.size()[0]\n",
        "        encoder_hidden = encoder.initHidden()\n",
        "\n",
        "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "        for ei in range(input_length):\n",
        "            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n",
        "                                                     encoder_hidden)\n",
        "            encoder_outputs[ei] += encoder_output[0, 0]\n",
        "\n",
        "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
        "\n",
        "        decoder_hidden = encoder_hidden\n",
        "\n",
        "        decoded_words = []\n",
        "\n",
        "        for di in range(max_length):\n",
        "            decoder_output, decoder_hidden = decoder(\n",
        "                decoder_input, decoder_hidden)\n",
        "            topv, topi = decoder_output.data.topk(1)\n",
        "            if topi.item() == EOS_token:\n",
        "                decoded_words.append('<EOS>')\n",
        "                break\n",
        "            else:\n",
        "                decoded_words.append(output_lang.index2word[topi.item()])\n",
        "\n",
        "            decoder_input = topi.squeeze().detach()\n",
        "\n",
        "        return decoded_words"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1qUmQIGwM9uf"
      },
      "source": [
        "def evaluateRandomly(encoder, decoder, n=10):\n",
        "    for i in range(n):\n",
        "        pair = random.choice(pairs)\n",
        "        print('>', pair[0])\n",
        "        print('=', pair[1])\n",
        "        output_words = evaluate(encoder, decoder, pair[0])\n",
        "        output_sentence = ' '.join(output_words)\n",
        "        print('<', output_sentence)\n",
        "        print('')"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Обучим  модель Seq2seq с использованием 1-слойных GRU для энкодера&декодера.С целью повышения качества предсказаний количество эпох увеличим вдвое (с 75000 до 150000)"
      ],
      "metadata": {
        "id": "GXRgoO7Nitdl"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s_56t10oM9uh",
        "outputId": "fc0ede62-3df5-4a0a-e60e-4331536c421c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "hidden_size = 256\n",
        "encoder1 = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
        "decoder1 = DecoderRNN(hidden_size, output_lang.n_words).to(device)\n",
        "\n",
        "trainIters(encoder1, decoder1, 150000, print_every=5000) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1m 0s (- 29m 0s) (5000 3%) 3.7474\n",
            "1m 51s (- 26m 4s) (10000 6%) 3.2412\n",
            "2m 44s (- 24m 43s) (15000 10%) 2.9723\n",
            "3m 37s (- 23m 33s) (20000 13%) 2.7699\n",
            "4m 30s (- 22m 31s) (25000 16%) 2.6018\n",
            "5m 22s (- 21m 31s) (30000 20%) 2.4844\n",
            "6m 15s (- 20m 33s) (35000 23%) 2.3738\n",
            "7m 8s (- 19m 37s) (40000 26%) 2.2868\n",
            "8m 1s (- 18m 42s) (45000 30%) 2.2007\n",
            "8m 54s (- 17m 48s) (50000 33%) 2.1600\n",
            "9m 46s (- 16m 53s) (55000 36%) 2.0674\n",
            "10m 39s (- 15m 59s) (60000 40%) 1.9800\n",
            "11m 33s (- 15m 6s) (65000 43%) 1.9497\n",
            "12m 26s (- 14m 12s) (70000 46%) 1.9243\n",
            "13m 19s (- 13m 19s) (75000 50%) 1.8637\n",
            "14m 13s (- 12m 26s) (80000 53%) 1.8398\n",
            "15m 6s (- 11m 32s) (85000 56%) 1.7801\n",
            "15m 58s (- 10m 39s) (90000 60%) 1.7514\n",
            "16m 51s (- 9m 45s) (95000 63%) 1.7061\n",
            "17m 44s (- 8m 52s) (100000 66%) 1.7122\n",
            "18m 37s (- 7m 58s) (105000 70%) 1.6542\n",
            "19m 30s (- 7m 5s) (110000 73%) 1.6396\n",
            "20m 23s (- 6m 12s) (115000 76%) 1.5780\n",
            "21m 17s (- 5m 19s) (120000 80%) 1.5571\n",
            "22m 9s (- 4m 25s) (125000 83%) 1.5464\n",
            "23m 2s (- 3m 32s) (130000 86%) 1.4969\n",
            "23m 55s (- 2m 39s) (135000 90%) 1.4576\n",
            "24m 48s (- 1m 46s) (140000 93%) 1.4540\n",
            "25m 41s (- 0m 53s) (145000 96%) 1.4499\n",
            "26m 34s (- 0m 0s) (150000 100%) 1.4227\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xEoEylSyM9uj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a92f4313-f663-4d2e-f9fd-a0b78ee3c9c8"
      },
      "source": [
        "evaluateRandomly(encoder1, decoder1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> ona vsio eshchio ego liubit .\n",
            "= she still loves him .\n",
            "< she still loves him . <EOS>\n",
            "\n",
            "> ona s nim surova .\n",
            "= she is hard on him .\n",
            "< she is hard on him . <EOS>\n",
            "\n",
            "> ty kogo nibud tam videl ?\n",
            "= did you see anybody there ?\n",
            "< did you see anyone there ? <EOS>\n",
            "\n",
            "> kak liudi tochili karandashi do izobreteniia tochilki ?\n",
            "= how did people sharpen their pencils before the invention of the pencil sharpener ?\n",
            "< how did the get get the of get ? <EOS>\n",
            "\n",
            "> vy izuchaete frantsuzskii ?\n",
            "= do you study french ?\n",
            "< do you study french ? <EOS>\n",
            "\n",
            "> mne nado znat seichas .\n",
            "= i have to know now .\n",
            "< i have to know now . <EOS>\n",
            "\n",
            "> ia sdelal eto dlia nas .\n",
            "= i did that for us .\n",
            "< i did it for for . <EOS>\n",
            "\n",
            "> on istekal krov iu ot ran .\n",
            "= he was bleeding from his wounds .\n",
            "< he is from from from . . <EOS>\n",
            "\n",
            "> kogda ty tuda doberesh sia ?\n",
            "= when are you going to get there ?\n",
            "< when do you get there ? <EOS>\n",
            "\n",
            "> ia sobiralsia zadat tebe tot zhe vopros .\n",
            "= i was about to ask you the same question .\n",
            "< i was the to ask you to ask . <EOS>\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Мы видим, что визуально качество перевода не очень хорошее и лосс гораздо больше 1.0. Возможно, при увеличении количества эпох ещё в несколько раз качество перевода удастся существенно улучшить, но эта операция очень ресурсоемкая: на GPU Google Colab (был выделен Tesla T4) обучение модели заняло ок.30 минут. Попробуем добавить по одному рекуррентному слою GRU в Encoder и Decoder (просто переопределим классы)."
      ],
      "metadata": {
        "id": "LPOrjm8a2jEu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## 2-layers GRU @Encoder\n",
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, n_layers):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        output = embedded\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        # return torch.zeros(1, 1, self.hidden_size, device=device)\n",
        "        return torch.zeros(2, 1, self.hidden_size, device=device)"
      ],
      "metadata": {
        "id": "N4S01SsB2VGp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## 2-layers GRU @Decoder\n",
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size, n_layers):\n",
        "        super(DecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        output = self.embedding(input).view(1, 1, -1)\n",
        "        output = F.relu(output)\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        output = self.softmax(self.out(output[0]))\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        # return torch.zeros(1, 1, self.hidden_size, device=device)\n",
        "        return torch.zeros(2, 1, self.hidden_size, device=device)"
      ],
      "metadata": {
        "id": "-j0emwd94BY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hidden_size = 256\n",
        "num_layers = 2\n",
        "encoder1 = EncoderRNN(input_lang.n_words, hidden_size, num_layers).to(device)\n",
        "decoder1 = DecoderRNN(hidden_size, output_lang.n_words, num_layers).to(device)\n",
        "\n",
        "trainIters(encoder1, decoder1, 150000, print_every=5000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HRLSiExI4n_g",
        "outputId": "5bad93a9-507c-4bb9-bd69-087d84571ba7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1m 10s (- 34m 18s) (5000 3%) 3.7949\n",
            "2m 12s (- 31m 1s) (10000 6%) 3.2709\n",
            "3m 14s (- 29m 12s) (15000 10%) 3.0179\n",
            "4m 17s (- 27m 51s) (20000 13%) 2.8252\n",
            "5m 20s (- 26m 41s) (25000 16%) 2.6823\n",
            "6m 23s (- 25m 32s) (30000 20%) 2.5338\n",
            "7m 25s (- 24m 25s) (35000 23%) 2.4270\n",
            "8m 29s (- 23m 19s) (40000 26%) 2.3474\n",
            "9m 31s (- 22m 13s) (45000 30%) 2.2012\n",
            "10m 34s (- 21m 8s) (50000 33%) 2.1370\n",
            "11m 37s (- 20m 4s) (55000 36%) 2.1030\n",
            "12m 39s (- 18m 59s) (60000 40%) 2.0296\n",
            "13m 42s (- 17m 55s) (65000 43%) 1.9408\n",
            "14m 45s (- 16m 52s) (70000 46%) 1.9263\n",
            "15m 48s (- 15m 48s) (75000 50%) 1.8574\n",
            "16m 51s (- 14m 45s) (80000 53%) 1.7943\n",
            "17m 54s (- 13m 41s) (85000 56%) 1.7575\n",
            "18m 57s (- 12m 38s) (90000 60%) 1.7528\n",
            "20m 0s (- 11m 35s) (95000 63%) 1.6581\n",
            "21m 3s (- 10m 31s) (100000 66%) 1.6490\n",
            "22m 6s (- 9m 28s) (105000 70%) 1.6144\n",
            "23m 10s (- 8m 25s) (110000 73%) 1.6010\n",
            "24m 13s (- 7m 22s) (115000 76%) 1.5472\n",
            "25m 16s (- 6m 19s) (120000 80%) 1.5584\n",
            "26m 19s (- 5m 15s) (125000 83%) 1.5142\n",
            "27m 22s (- 4m 12s) (130000 86%) 1.5065\n",
            "28m 26s (- 3m 9s) (135000 90%) 1.4539\n",
            "29m 29s (- 2m 6s) (140000 93%) 1.4945\n",
            "30m 32s (- 1m 3s) (145000 96%) 1.4015\n",
            "31m 35s (- 0m 0s) (150000 100%) 1.3766\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evaluateRandomly(encoder1, decoder1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g7pj6e2UKU0E",
        "outputId": "355ddc9b-5067-47e2-c7f0-2ab999d5ccff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> ty deistvitel no zdorovo vygliadish .\n",
            "= you really do look great .\n",
            "< you really are look beautiful . <EOS>\n",
            "\n",
            "> mne skazali chto tom uekhal za granitsu .\n",
            "= i was told that tom went abroad .\n",
            "< i was told tom was interested in . <EOS>\n",
            "\n",
            "> kak skoro ty mozhesh zakonchit eti risunki ?\n",
            "= how quickly can you finish these pictures ?\n",
            "< how soon can you be such my hands ? <EOS>\n",
            "\n",
            "> u tebia eshchio est voprosy ?\n",
            "= do you still have questions ?\n",
            "< do you have any more ? <EOS>\n",
            "\n",
            "> khotite chtoby ia ostalsia zdes ?\n",
            "= do you want me to stay here ?\n",
            "< do you want me to stay here ? <EOS>\n",
            "\n",
            "> tebe ponravilos ili net ?\n",
            "= did you like that or not ?\n",
            "< did you like that or not ? <EOS>\n",
            "\n",
            "> kak chasto ty pol zuesh sia kameroi na svoiom smartfone ?\n",
            "= how often do you use the camera on your smartphone ?\n",
            "< how often do you use your your ? <EOS>\n",
            "\n",
            "> ty znala chto u tebia krasivye glaza ?\n",
            "= did you know you have pretty eyes ?\n",
            "< did you know that you you you ? ? ? <EOS>\n",
            "\n",
            "> ia gotov sdelat chto ugodno chtoby pomoch tebe .\n",
            "= i am ready to do anything to help you .\n",
            "< i am ready to do anything to help help . <EOS>\n",
            "\n",
            "> on poslednii chelovek s kem ia khochu razgovarivat .\n",
            "= he is the last man that i want to talk with .\n",
            "< he is the last person i want to go to see . <EOS>\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Мы видим, что качество перевода с использованием 2х-слойных GRU для энкодера/декодера несущественно улучшилось (лосс=1.3766 на 2х-слойных против лосса=1.4227 на 1-слойных). Время обучения модели при этом практически не увеличилось.\n",
        "\n",
        "## Попробуем теперь заменить 2х-слойную GRU на 1-слойную LSTM."
      ],
      "metadata": {
        "id": "jQr9JJYeRsyF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# class EncoderLSTM_test(nn.Module):\n",
        "#     def init(self, inputsize, hiddensize):\n",
        "#       super(EncoderLSTM_test, self).init()\n",
        "#       self.hiddensize = hiddensize\n",
        "\n",
        "#       self.embedding = nn.Embedding(inputsize, hidden_size)\n",
        "#       self.lstm = nn.LSTM(hidden_size, hidden_size)\n",
        "\n",
        "#     def forward(self, input, hidden):\n",
        "#       embedded = self.embedding(input).view(1, 1, -1)\n",
        "#       output = embedded\n",
        "#       output, hidden = self.lstm(output, hidden)\n",
        "#       return output, hidden\n",
        "\n",
        "#     def initHidden(self):\n",
        "#       return torch.zeros(1, 1, self.hidden_size), torch.zeros(1, 1, self.hidden_size)"
      ],
      "metadata": {
        "id": "7eRe_svySjL1"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderLSTM_new(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(EncoderLSTM_new, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.lstm = nn.LSTM(hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        output = embedded\n",
        "        output, hidden = self.lstm(output, hidden)\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size).to(device), torch.zeros(1, 1, self.hidden_size).to(device)"
      ],
      "metadata": {
        "id": "KQGLSgqLS4VE"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderLSTM_new(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size):\n",
        "        super(DecoderLSTM_new, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "        self.lstm = nn.LSTM(hidden_size, hidden_size)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        output = self.embedding(input).view(1, 1, -1)\n",
        "        output = F.relu(output)\n",
        "        output, hidden = self.lstm(output, hidden)\n",
        "        output = self.softmax(self.out(output[0]))\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size).to(device), torch.zeros(1, 1, self.hidden_size).to(device)"
      ],
      "metadata": {
        "id": "vImLOlLyT_mg"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hidden_size = 256\n",
        "encoder1 = EncoderLSTM_new(input_lang.n_words, hidden_size).to(device)\n",
        "decoder1 = DecoderLSTM_new(hidden_size, output_lang.n_words).to(device)\n",
        "\n",
        "trainIters(encoder1, decoder1, 150000, print_every=5000) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PK5J38P3XE_m",
        "outputId": "10dbde06-edaf-405e-e18a-f945e0a0cb5c"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1m 4s (- 31m 2s) (5000 3%) 3.8467\n",
            "1m 59s (- 27m 55s) (10000 6%) 3.4887\n",
            "2m 54s (- 26m 8s) (15000 10%) 3.1790\n",
            "3m 49s (- 24m 52s) (20000 13%) 2.9905\n",
            "4m 44s (- 23m 40s) (25000 16%) 2.8287\n",
            "5m 42s (- 22m 51s) (30000 20%) 2.7193\n",
            "6m 38s (- 21m 48s) (35000 23%) 2.6156\n",
            "7m 33s (- 20m 46s) (40000 26%) 2.5188\n",
            "8m 28s (- 19m 46s) (45000 30%) 2.4546\n",
            "9m 24s (- 18m 48s) (50000 33%) 2.3970\n",
            "10m 18s (- 17m 48s) (55000 36%) 2.3431\n",
            "11m 13s (- 16m 50s) (60000 40%) 2.2334\n",
            "12m 8s (- 15m 53s) (65000 43%) 2.2143\n",
            "13m 3s (- 14m 55s) (70000 46%) 2.1594\n",
            "14m 3s (- 14m 3s) (75000 50%) 2.0957\n",
            "14m 58s (- 13m 6s) (80000 53%) 2.0278\n",
            "15m 54s (- 12m 9s) (85000 56%) 1.9983\n",
            "16m 49s (- 11m 13s) (90000 60%) 1.9601\n",
            "17m 45s (- 10m 17s) (95000 63%) 1.9030\n",
            "18m 41s (- 9m 20s) (100000 66%) 1.8662\n",
            "19m 36s (- 8m 24s) (105000 70%) 1.8393\n",
            "20m 31s (- 7m 27s) (110000 73%) 1.7972\n",
            "21m 33s (- 6m 33s) (115000 76%) 1.7636\n",
            "22m 29s (- 5m 37s) (120000 80%) 1.7351\n",
            "23m 24s (- 4m 40s) (125000 83%) 1.7025\n",
            "24m 19s (- 3m 44s) (130000 86%) 1.6714\n",
            "25m 14s (- 2m 48s) (135000 90%) 1.6042\n",
            "26m 10s (- 1m 52s) (140000 93%) 1.6037\n",
            "27m 5s (- 0m 56s) (145000 96%) 1.5858\n",
            "28m 0s (- 0m 0s) (150000 100%) 1.5499\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evaluateRandomly(encoder1, decoder1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7uavMfN3hC8b",
        "outputId": "2ec8d9d5-2fdd-4b3d-cb75-6eaacd160035"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> kak ia vygliazhu ?\n",
            "= how do i look ?\n",
            "< how do i get ? <EOS>\n",
            "\n",
            "> tom ne zabyl sdelat to o chiom my ego prosili ?\n",
            "= did tom remember to do what we asked him to do ?\n",
            "< did tom want to do what what do asked to do ? <EOS>\n",
            "\n",
            "> mne net ravnykh .\n",
            "= i have no equal .\n",
            "< i have no . . <EOS>\n",
            "\n",
            "> ia byl slishkom slab chtoby vstat s posteli .\n",
            "= i was too weak to get out of bed .\n",
            "< i was too tired to get out of bed . <EOS>\n",
            "\n",
            "> on deistvitel no dzhentl men .\n",
            "= he is quite a gentleman .\n",
            "< he is really that . <EOS>\n",
            "\n",
            "> ia nadeialsia chto smogu pomoch tomu eto sdelat .\n",
            "= i was hoping i could help tom do that .\n",
            "< i was hoping we could help do that . <EOS>\n",
            "\n",
            "> ty eto pomnish ?\n",
            "= do you remember that ?\n",
            "< do you remember this ? <EOS>\n",
            "\n",
            "> kak eto ty tak zdorovo poranilsia ?\n",
            "= how did you get hurt so badly ?\n",
            "< how did you get so so so ? <EOS>\n",
            "\n",
            "> ty obiazan eto delat ?\n",
            "= are you obliged to do that ?\n",
            "< did you intend to do that ? <EOS>\n",
            "\n",
            "> klianiotes li vy govorit pravdu i nichego krome pravdy ?\n",
            "= do you swear to tell the truth and nothing but the truth ?\n",
            "< are you telling to tell and and and tell the truth ? <EOS>\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Мы видим, что на 1-слойной LSTM качество перевода ухудшилось как визуально, так и на основании лосса (1.5499). Время обучения (28 мин.) сопоставимо с GRU.\n",
        "\n",
        "## Наилучшее качество было на 2х-слойной GRU (лосс 1.3766), чуть хуже - на 1-слойной GRU (лосс 1.4227). \n",
        "\n",
        "## Полагаю, что на LSTM удалось бы существенно улучшить качество, добавив ещё слой LSTM и сделав сеть двунаправленной."
      ],
      "metadata": {
        "id": "SkogsYbdhXTH"
      }
    }
  ]
}